{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader as DL\n",
    "from torch.utils.data import TensorDataset as TData\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "! unzip \"/content/drive/MyDrive/BCICIV_2a_all_patients.csv.zip\"\n",
    "dataset = pd.read_csv(\"/content/drive/MyDrive/BCICIV_2a_all_patients.csv.zip\") # loading in the csv dataset with pandas\n",
    "dataset = dataset.drop(columns=['EEG-Fz','EEG-2', 'EEG-14', 'EEG-Pz', 'EEG-15', 'EEG-16'])\n",
    "new_order = ['patient', 'time', 'label', 'epoch', 'EEG-5', 'EEG-0', 'EEG-C3', 'EEG-9',\n",
    "       'EEG-10', 'EEG-6', 'EEG-1', 'EEG-Cz', 'EEG-11', 'EEG-12',\n",
    "      'EEG-7', 'EEG-3', 'EEG-4', 'EEG-C4', 'EEG-13', 'EEG-8']\n",
    "dataset = dataset[new_order]\n",
    "#may need to change this when more degrees of freedom are added\n",
    "binary_mi = dataset.loc[(dataset['label'] == 'left') | (dataset['label'] == 'right')]\n",
    "binary_mi.label.unique() # check that our row filter worked\n",
    "# get time period and check that it is constant throughout dataframe\n",
    "t0 = binary_mi.time.iloc[:-1] # get time points except last one\n",
    "t1 = binary_mi.time.iloc[1:] # series offest by one, so starting at 2nd time point until last\n",
    "t0.reset_index(drop=True, inplace=True)\n",
    "t1.reset_index(drop=True, inplace=True)\n",
    "const_diff = ((t0 - t1 < -0.0039) & (t0 - t1 > -0.0041)) # checking that they are all around -0.004\n",
    "const_diff[const_diff != True] # getting just the rows where time difference wasn't around -0.004\n",
    "binary_mi.iloc[200:, 2] # time discrepancy seems to be caused by label change\n",
    "pruned_mi = binary_mi.drop(columns = ['time', 'epoch'])\n",
    "patients = binary_mi.patient.unique() # 1-9\n",
    "labels = binary_mi.label.unique()     # left and right\n",
    "left_hand = []\n",
    "right_hand = []\n",
    "\n",
    "# for each patient, get patient readings, convert to numpy array, and add\n",
    "# to list corresponding to target (left or right)\n",
    "for patient in patients:\n",
    "  left_df = pruned_mi[(pruned_mi['patient'] == patient) & (pruned_mi['label'] == 'left')]\n",
    "  right_df = pruned_mi[(pruned_mi['patient'] == patient) & (pruned_mi['label'] == 'right')]\n",
    "\n",
    "  left_hand.append(left_df.iloc[:, 2:].to_numpy().T)\n",
    "  right_hand.append(right_df.iloc[:, 2:].to_numpy().T)\n",
    "\n",
    "right_hand[0].shape # shows shape of each list element\n",
    "# get sequence length/length of each signal\n",
    "left_len = left_hand[0].shape[1]\n",
    "right_len = right_hand[0].shape[1]\n",
    "# portion of sequence used for validation/testing set\n",
    "left_eval_len = int(0.4 * left_hand[0].shape[1])\n",
    "right_eval_len = int(0.4 * right_hand[0].shape[1])\n",
    "\n",
    "# select random starting indexes for sub-sequences for validation and testing sets\n",
    "left_hand_partition = np.random.randint(0, left_len - left_eval_len, len(left_hand))\n",
    "right_hand_partition = np.random.randint(0, right_len - right_eval_len, len(right_hand))\n",
    "\n",
    "# initialize lists to hold respective signals\n",
    "left_train = []\n",
    "left_val = []\n",
    "left_test = []\n",
    "right_train = []\n",
    "right_val = []\n",
    "right_test = []\n",
    "\n",
    "# create training, validation, and testing sets for left hand signals\n",
    "for i, num in enumerate(left_hand_partition):\n",
    "  # get ending index of subsequence being cropped for validation and testing sets\n",
    "  upper_end = num + left_eval_len\n",
    "  # add portion of sequence excluding num:upper_end (portion used for evaluation)\n",
    "  left_train.append(left_hand[i][:, list(range(num)) + list(range(upper_end, left_len))])\n",
    "  # first half of subsequence used for validation\n",
    "  left_val.append(left_hand[i][:, num:num+(left_eval_len//2)])\n",
    "  # second half used for test set\n",
    "  left_test.append(left_hand[i][:, num+(left_eval_len//2):num + left_eval_len])\n",
    "\n",
    "# create training, validation, and testing sets for right hand signals\n",
    "for i, num in enumerate(right_hand_partition):\n",
    "  # get ending index of subsequence being cropped for validation and testing sets\n",
    "  upper_end = num + right_eval_len\n",
    "  # add portion of sequence excluding num:upper_end (portion used for evaluation)\n",
    "  right_train.append(right_hand[i][:, list(range(num)) + list(range(upper_end, right_len))])\n",
    "  # first half of subsequence used for validation\n",
    "  right_val.append(right_hand[i][:, num:num+(right_eval_len//2)])\n",
    "  # second half used for test set\n",
    "  right_test.append(right_hand[i][:, num+(right_eval_len//2):num + right_eval_len])\n",
    "\n",
    "# function to bandpass filter signals\n",
    "def bandpass_filter(signal, crit_freq = [5, 40], sampling_freq = 125):\n",
    "  order = 4\n",
    "  b, a = scipy.signal.butter(order, crit_freq, btype = 'bandpass', fs = sampling_freq)\n",
    "  processed_signal = scipy.signal.filtfilt(b, a, signal, 1)\n",
    "  return processed_signal\n",
    "\n",
    "# segment a signal using sliding window technique, specifying sample frequency, window size, and window shift\n",
    "def segmentation(signal, sampling_freq=125, window_size=1, window_shift=0.016):\n",
    "  w_size = int(sampling_freq * window_size)\n",
    "  w_shift = int(sampling_freq * window_shift)\n",
    "  segments = []\n",
    "  i = 0\n",
    "  while i + w_size <= signal.shape[1]:\n",
    "    segments.append(signal[:, i: i + w_size])\n",
    "    i += w_shift\n",
    "  return segments\n",
    "\n",
    "# apply preprocessing steps in sequence to each signal\n",
    "def preprocess(signals, crit_freq=[5,35], fs=250):\n",
    "  preprocessed = []\n",
    "  for signal in signals:\n",
    "    # perform bandpass filter on each signal\n",
    "    filtered_signal = bandpass_filter(signal, crit_freq, fs)\n",
    "    # channel-wise z-score normalization\n",
    "    normed_signal = (filtered_signal - filtered_signal.mean(1, keepdims=True)) / filtered_signal.std(1, keepdims=True)\n",
    "    # segmentation of signals\n",
    "    segments = segmentation(normed_signal, fs)\n",
    "    # add signals to list\n",
    "    preprocessed.extend(segments)\n",
    "  return preprocessed\n",
    "# generate preprocessed segments for training, validation, and testing\n",
    "train_left = preprocess(left_train)\n",
    "val_left = preprocess(left_val)\n",
    "test_left = preprocess(left_test)\n",
    "train_right = preprocess(right_train)\n",
    "val_right = preprocess(right_val)\n",
    "test_right = preprocess(right_test)\n",
    "# combine left and right signals to generate signal training, validation, and testing set\n",
    "train_eeg = train_left + train_right\n",
    "val_eeg = val_left + val_right\n",
    "test_eeg = test_left + test_right\n",
    "train_labels = [0 for i in range(len(train_left))] + [1 for i in range(len(train_right))]\n",
    "val_labels = [0 for i in range(len(val_left))] + [1 for i in range(len(val_right))]\n",
    "test_labels = [0 for i in range(len(test_left))] + [1 for i in range(len(test_right))]\n",
    "# create torch tensor of zeros to hold data\n",
    "train_eeg_tensor = torch.zeros((len(train_eeg), train_eeg[0].shape[0], train_eeg[0].shape[1]))\n",
    "valid_eeg_tensor = torch.zeros((len(val_eeg), val_eeg[0].shape[0], val_eeg[0].shape[1]))\n",
    "test_eeg_tensor = torch.zeros((len(test_eeg), test_eeg[0].shape[0], test_eeg[0].shape[1]))\n",
    "\n",
    "# add each sample in train, validation, and test lists to appropriate tensor at correct index\n",
    "for i in range(len(train_eeg)):\n",
    "  tens = torch.from_numpy(train_eeg[i].copy())\n",
    "  train_eeg_tensor[i] = tens\n",
    "for i in range(len(val_eeg)):\n",
    "  tens = torch.from_numpy(val_eeg[i].copy())\n",
    "  valid_eeg_tensor[i] = tens\n",
    "for i in range(len(test_eeg)):\n",
    "  tens = torch.from_numpy(test_eeg[i].copy())\n",
    "  test_eeg_tensor[i] = tens\n",
    "\n",
    "# create zero tensor for one hot encoded labels\n",
    "#train_labels should now equal 12\n",
    "train_label_tensor = torch.zeros(len(train_labels), 2)\n",
    "valid_label_tensor = torch.zeros(len(val_labels), 2)\n",
    "test_label_tensor = torch.zeros(len(test_labels), 2)\n",
    "\n",
    "\n",
    "# add labels to tensor at correct index\n",
    "for i in range(len(train_labels)):\n",
    "  label = train_labels[i]\n",
    "  train_label_tensor[i][label] = 1\n",
    "for i in range(len(val_labels)):\n",
    "  label = val_labels[i]\n",
    "  valid_label_tensor[i][label] = 1\n",
    "for i in range(len(test_labels)):\n",
    "  label = test_labels[i]\n",
    "  test_label_tensor[i][label] = 1\n",
    "\n",
    "# convert input, target tensors to Tensor Dataset from torch\n",
    "train_ds = TData(train_eeg_tensor, train_label_tensor)\n",
    "valid_ds = TData(valid_eeg_tensor, valid_label_tensor)\n",
    "test_ds = TData(test_eeg_tensor, test_label_tensor)\n",
    "# create dataloaders to hold batched data (batch size chosen was 64)\n",
    "train_dl = DL(train_ds, batch_size=64, shuffle= True, drop_last = True)\n",
    "valid_dl = DL(valid_ds, batch_size=64, shuffle= True, drop_last = True)\n",
    "test_dl = DL(test_ds, batch_size=64, shuffle = True, drop_last = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LHNT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
