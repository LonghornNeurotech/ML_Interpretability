{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqcjUC3bOKhu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import scipy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader as DL\n",
        "from torch.utils.data import TensorDataset as TData\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import pickle\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4PgfyyOjP6",
        "outputId": "1745ae8e-7b94-4391-958b-3f523e688c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IECB-h2uXL-p",
        "outputId": "3042c0bd-ac64-4403-e5ff-263cf780a7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Wed Apr 16 22:58:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYjr4RAoVUGu"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzU-bWmr8A4E",
        "outputId": "ee66d80f-9c47-4d28-8249-915bec1ee907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/LHNT_Individual_Data.zip\n",
            "replace LHNT EEG/Alan_Fletcher_Session4.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session4.zip  \n",
            "replace LHNT EEG/Ademola_Adetosoye_Session1.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Ademola_Adetosoye_Session1.0.zip  \n",
            "replace LHNT EEG/Alex_Xie_Session1.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alex_Xie_Session1.zip  \n",
            "replace LHNT EEG/Alex_Xie_Session2.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alex_Xie_Session2.zip  \n",
            "replace LHNT EEG/Alan_Fletcher_Session2.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session2.zip  \n",
            "replace LHNT EEG/Alex_Xie_Session3.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alex_Xie_Session3.zip  \n",
            "replace LHNT EEG/Alan_Fletcher_Session1.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session1.zip  \n",
            "replace LHNT EEG/Alan_Fletcher_Session3.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alan_Fletcher_Session3.zip  \n",
            "replace LHNT EEG/Alex_Xie_Session4.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Alex_Xie_Session4.zip  \n",
            "replace LHNT EEG/Jane_Doe_Session407.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Jane_Doe_Session407.0.zip  \n",
            "replace LHNT EEG/jimmy_neutron_Session3.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/jimmy_neutron_Session3.0.zip  \n",
            "replace LHNT EEG/jimmy_neutron_Session4.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/jimmy_neutron_Session4.0.zip  \n",
            "replace LHNT EEG/Maddox_Fletcher_Session1.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Maddox_Fletcher_Session1.0.zip  \n",
            "replace LHNT EEG/morgan_dye_Session1.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace LHNT EEG/morgan_dye_Session1.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: yy\n",
            "  inflating: LHNT EEG/morgan_dye_Session1.zip  \n",
            "replace LHNT EEG/morgan_dye_Session2.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/morgan_dye_Session2.zip  \n",
            "replace LHNT EEG/morgan_dye_Session3.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/morgan_dye_Session3.zip  \n",
            "replace LHNT EEG/nandini_senthilkumar_Session1.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session1.zip  \n",
            "replace LHNT EEG/nandini_senthilkumar_Session2.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session2.zip  \n",
            "replace LHNT EEG/nandini_senthilkumar_Session3.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session3.zip  \n",
            "replace LHNT EEG/nandini_senthilkumar_Session4.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/nandini_senthilkumar_Session4.zip  \n",
            "replace LHNT EEG/Nandini_Senthilkumar_Session5.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Nandini_Senthilkumar_Session5.zip  \n",
            "replace LHNT EEG/Nandini_Senthilkumar_Session6.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Nandini_Senthilkumar_Session6.zip  \n",
            "replace LHNT EEG/Samarth_Rao_Session1.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Samarth_Rao_Session1.0.zip  \n",
            "replace LHNT EEG/Samarth_Rao_Session2.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Samarth_Rao_Session2.0.zip  \n",
            "replace LHNT EEG/Samarth_Rao_Session3.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Samarth_Rao_Session3.0.zip  \n",
            "replace LHNT EEG/Samarth_Rao_Session4.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Samarth_Rao_Session4.0.zip  \n",
            "replace LHNT EEG/Samarth_Rao_Session5.0.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/Samarth_Rao_Session5.0.zip  \n",
            "replace LHNT EEG/user_table.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: LHNT EEG/user_table.csv  \n"
          ]
        }
      ],
      "source": [
        "! unzip \"/content/LHNT_Individual_Data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBcJo5t0-Bu-",
        "outputId": "fbe640a2-9986-498b-e331-8576d593856e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/LHNT EEG/morgan_dye_Session1.zip\n",
            "   creating: morgan_dye_Session1/\n",
            "  inflating: morgan_dye_Session1/left_1.pkl  \n",
            "  inflating: morgan_dye_Session1/left_15.pkl  \n",
            "  inflating: morgan_dye_Session1/left_17.pkl  \n",
            "  inflating: morgan_dye_Session1/left_3.pkl  \n",
            "  inflating: morgan_dye_Session1/left_7.pkl  \n",
            "  inflating: morgan_dye_Session1/left_13.pkl  \n",
            "  inflating: morgan_dye_Session1/left_11.pkl  \n",
            "  inflating: morgan_dye_Session1/left_5.pkl  \n",
            "  inflating: morgan_dye_Session1/right_2.pkl  \n",
            "  inflating: morgan_dye_Session1/right_18.pkl  \n",
            "  inflating: morgan_dye_Session1/right_20.pkl  \n",
            "  inflating: morgan_dye_Session1/right_4.pkl  \n",
            "  inflating: morgan_dye_Session1/right_6.pkl  \n",
            "  inflating: morgan_dye_Session1/right_12.pkl  \n",
            "  inflating: morgan_dye_Session1/right_10.pkl  \n",
            "  inflating: morgan_dye_Session1/right_8.pkl  \n",
            "  inflating: morgan_dye_Session1/right_14.pkl  \n",
            "  inflating: morgan_dye_Session1/right_16.pkl  \n",
            "  inflating: morgan_dye_Session1/left_9.pkl  \n",
            "  inflating: morgan_dye_Session1/left_19.pkl  \n",
            "Archive:  /content/LHNT EEG/morgan_dye_Session2.zip\n",
            "   creating: morgan_dye_Session2/\n",
            "  inflating: morgan_dye_Session2/left_1.pkl  \n",
            "  inflating: morgan_dye_Session2/left_15.pkl  \n",
            "  inflating: morgan_dye_Session2/left_17.pkl  \n",
            "  inflating: morgan_dye_Session2/left_3.pkl  \n",
            "  inflating: morgan_dye_Session2/left_7.pkl  \n",
            "  inflating: morgan_dye_Session2/left_13.pkl  \n",
            "  inflating: morgan_dye_Session2/left_11.pkl  \n",
            "  inflating: morgan_dye_Session2/left_5.pkl  \n",
            "  inflating: morgan_dye_Session2/right_2.pkl  \n",
            "  inflating: morgan_dye_Session2/right_18.pkl  \n",
            "  inflating: morgan_dye_Session2/right_20.pkl  \n",
            "  inflating: morgan_dye_Session2/right_4.pkl  \n",
            "  inflating: morgan_dye_Session2/right_6.pkl  \n",
            "  inflating: morgan_dye_Session2/right_12.pkl  \n",
            "  inflating: morgan_dye_Session2/right_10.pkl  \n",
            "  inflating: morgan_dye_Session2/right_8.pkl  \n",
            "  inflating: morgan_dye_Session2/right_14.pkl  \n",
            "  inflating: morgan_dye_Session2/right_16.pkl  \n",
            "  inflating: morgan_dye_Session2/left_9.pkl  \n",
            "  inflating: morgan_dye_Session2/left_19.pkl  \n",
            "Archive:  /content/LHNT EEG/morgan_dye_Session3.zip\n",
            "   creating: morgan_dye_Session3/\n",
            "  inflating: morgan_dye_Session3/left_1.pkl  \n",
            "  inflating: morgan_dye_Session3/left_15.pkl  \n",
            "  inflating: morgan_dye_Session3/left_17.pkl  \n",
            "  inflating: morgan_dye_Session3/left_3.pkl  \n",
            "  inflating: morgan_dye_Session3/left_7.pkl  \n",
            "  inflating: morgan_dye_Session3/left_13.pkl  \n",
            "  inflating: morgan_dye_Session3/left_11.pkl  \n",
            "  inflating: morgan_dye_Session3/left_5.pkl  \n",
            "  inflating: morgan_dye_Session3/right_2.pkl  \n",
            "  inflating: morgan_dye_Session3/right_18.pkl  \n",
            "  inflating: morgan_dye_Session3/right_20.pkl  \n",
            "  inflating: morgan_dye_Session3/right_4.pkl  \n",
            "  inflating: morgan_dye_Session3/right_6.pkl  \n",
            "  inflating: morgan_dye_Session3/right_12.pkl  \n",
            "  inflating: morgan_dye_Session3/right_10.pkl  \n",
            "  inflating: morgan_dye_Session3/right_8.pkl  \n",
            "  inflating: morgan_dye_Session3/right_14.pkl  \n",
            "  inflating: morgan_dye_Session3/right_16.pkl  \n",
            "  inflating: morgan_dye_Session3/left_9.pkl  \n",
            "  inflating: morgan_dye_Session3/left_19.pkl  \n"
          ]
        }
      ],
      "source": [
        "! unzip \"/content/LHNT EEG/morgan_dye_Session1.zip\"\n",
        "! unzip \"/content/LHNT EEG/morgan_dye_Session2.zip\"\n",
        "! unzip \"/content/LHNT EEG/morgan_dye_Session3.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLZKhGs5vBCk"
      },
      "outputs": [],
      "source": [
        "root = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSYL5T_lOKhw"
      },
      "outputs": [],
      "source": [
        "# Base folder containing the files\n",
        "# right_session_folder = os.path.join(root, \"alan_f_right/session_1\")\n",
        "\n",
        "# Initialize a dictionary to store the loaded signals\n",
        "right_eeg_data = {}\n",
        "\n",
        "# Iterate through files labeled in increments of two\n",
        "for j in range(1, 4):\n",
        "  right_session_folder = os.path.join(root, f\"morgan_dye_Session{j}\")\n",
        "  for i in range(2, 21, 2):  # Adjust the range as needed\n",
        "\n",
        "      file_path = os.path.join(right_session_folder, f\"right_16.pkl\")\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "          with open(file_path, \"rb\") as file:\n",
        "              data = pickle.load(file)\n",
        "              right_eeg_data[f\"right_{j}_{i}\"] = data  # Store the data with the corresponding label\n",
        "      else:\n",
        "          print(f\"File not found: {file_path}\")\n",
        "          break  # Stop if the file sequence ends\n",
        "\n",
        "left_eeg_data = {}\n",
        "\n",
        "for j in range(1, 4):\n",
        "  left_session_folder = os.path.join(root, f\"morgan_dye_Session{j}\")\n",
        "  # Iterate through files labeled in increments of two\n",
        "  for i in range(1, 21, 2):  # Adjust the range as needed\n",
        "      file_path = os.path.join(left_session_folder, f\"left_19.pkl\")\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "          with open(file_path, \"rb\") as file:\n",
        "              data = pickle.load(file)\n",
        "              left_eeg_data[f\"left_{j}_{i}\"] = data  # Store the data with the corresponding label\n",
        "      else:\n",
        "          print(f\"File not found: {file_path}\")\n",
        "          break  # Stop if the file sequence ends\n",
        "\n",
        "\n",
        "\n",
        "right_signal_data = {label: signal[0] for label, signal in right_eeg_data.items()}\n",
        "right_metadata = {label: signal[1] for label, signal in right_eeg_data.items()}\n",
        "\n",
        "left_signal_data = {label: signal[0] for label, signal in left_eeg_data.items()}\n",
        "metadata = {label: signal[1] for label, signal in left_eeg_data.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qicPbkMivQuo"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def baseline_correction(signal):\n",
        "    \"\"\"\n",
        "    Removes baseline offset by subtracting the mean of each channel.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Baseline-corrected signal.\n",
        "    \"\"\"\n",
        "    return signal - np.mean(signal, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def bandpass_filter(signal, lowcut=13, highcut=30, fs=125, order=4):\n",
        "    \"\"\"\n",
        "    Band-pass filters the signal for the specified frequency range.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "        lowcut (float): Lower cutoff frequency (Hz).\n",
        "        highcut (float): Upper cutoff frequency (Hz).\n",
        "        fs (float): Sampling rate (Hz).\n",
        "        order (int): Order of the filter.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Band-pass filtered signal.\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype=\"band\")\n",
        "    return filtfilt(b, a, signal, axis=1)\n",
        "\n",
        "def normalize_signal(signal):\n",
        "    \"\"\"\n",
        "    Normalizes the signal for each channel (z-score normalization).\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized signal.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(signal.T).T  # Transpose to normalize each channel\n",
        "\n",
        "def preprocess_eeg(signal, fs=256):\n",
        "    \"\"\"\n",
        "    Preprocess EEG signal with baseline correction, band-pass filtering, and normalization.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "        fs (float): Sampling rate (Hz).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Preprocessed EEG signal.\n",
        "    \"\"\"\n",
        "    # Step 1: Baseline Correction\n",
        "    signal_corrected = baseline_correction(signal)\n",
        "\n",
        "    # Step 2: Band-Pass Filter (Beta Frequencies)\n",
        "    signal_filtered = bandpass_filter(signal_corrected, lowcut=4, highcut=40, fs=fs)\n",
        "\n",
        "    # Step 3: Normalization\n",
        "    signal_normalized = normalize_signal(signal_filtered)\n",
        "\n",
        "    return signal_normalized\n",
        "\n",
        "def sliding_window_augmentation(data, labels, window_size=125, stride=80):\n",
        "    \"\"\"\n",
        "    Apply sliding window to already EMD-processed data\n",
        "\n",
        "    Args:\n",
        "    data: numpy array of shape (n_samples, n_channels*n_imfs, time_steps)\n",
        "    labels: numpy array of shape (n_samples,)\n",
        "    window_size: size of sliding window\n",
        "    stride: step size for sliding window\n",
        "\n",
        "    Returns:\n",
        "    augmented_data, augmented_labels\n",
        "    \"\"\"\n",
        "    n_samples, n_features, time_steps = data.shape\n",
        "    n_windows = (time_steps - window_size) // stride + 1\n",
        "\n",
        "    augmented_data = np.zeros((n_samples * n_windows, n_features, window_size))\n",
        "    augmented_labels = np.zeros(n_samples * n_windows, dtype=labels.dtype)\n",
        "\n",
        "    idx = 0\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_windows):\n",
        "            start = j * stride\n",
        "            end = start + window_size\n",
        "            augmented_data[idx] = data[i, :, start:end]\n",
        "            augmented_labels[idx] = labels[i]\n",
        "            idx += 1\n",
        "\n",
        "    return augmented_data, augmented_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyOEe9eKvTMM",
        "outputId": "4701e2b9-9639-4be0-9b3d-b09b35af7505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right_1_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "left_1_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_19: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_19: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_19: LEFT Processed Signal Shape = (16, 875)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess each signal in the dictionary\n",
        "right_preprocessed_signals = {label: preprocess_eeg(signal, fs=256) for label, signal in right_signal_data.items()}\n",
        "\n",
        "# Inspect the shapes of the processed signals\n",
        "for label, signal in right_preprocessed_signals.items():\n",
        "    print(f\"{label}: RIGHT Processed Signal Shape = {signal.shape}\")\n",
        "\n",
        "\n",
        "# Preprocess each signal in the dictionary\n",
        "left_preprocessed_signals = {label: preprocess_eeg(signal, fs=256) for label, signal in left_signal_data.items()}\n",
        "\n",
        "# Inspect the shapes of the processed signals\n",
        "for label, signal in left_preprocessed_signals.items():\n",
        "    print(f\"{label}: LEFT Processed Signal Shape = {signal.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b5NTIexvfJ7"
      },
      "outputs": [],
      "source": [
        "left_labels = np.array([0 for _ in range(len(left_preprocessed_signals))])\n",
        "right_labels = np.array([1 for _ in range(len(right_preprocessed_signals))])\n",
        "\n",
        "left, left_labels = sliding_window_augmentation(np.array(list(left_preprocessed_signals.values())), np.array(left_labels), window_size=80, stride=60)\n",
        "right, right_labels = sliding_window_augmentation(np.array(list(right_preprocessed_signals.values())), np.array(right_labels), window_size=80, stride=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaol39KVVP36"
      },
      "source": [
        "# Data Augmentation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdJQRTVLv0IE"
      },
      "source": [
        "# model things\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkbYjcWNwUGs"
      },
      "outputs": [],
      "source": [
        "# run this if no EMD!\n",
        "augmented_data = np.concatenate((left, right))\n",
        "augmented_labels = np.concatenate((left_labels, right_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZE4KRzso-q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMD_PCNN(nn.Module):\n",
        "    def __init__(self, input_channels=16, num_classes=2, input_length=80):\n",
        "        super().__init__()\n",
        "\n",
        "        # Branch 1 (input_length = 125)\n",
        "        # Branch 1 (input_length = 125)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=20, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),  # Output length: 125 // 2 = 62\n",
        "            # nn.Conv1d(16, 32, kernel_size=10, padding='same'),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.MaxPool1d(2)  # Output length: 62 // 2 = 31\n",
        "        )\n",
        "\n",
        "        # Branch 2 (input_length = 125)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=10, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),  # Output length: 125 // 2 = 62\n",
        "            # nn.Conv1d(16, 32, kernel_size=5, padding='same'),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.MaxPool1d(2)  # Output length: 62 // 2 = 31\n",
        "        )\n",
        "\n",
        "        # Calculate FC input dimension dynamically\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1(x)  # Shape: (batch, 32, 31)\n",
        "        out2 = self.branch2(x)  # Shape: (batch, 32, 31)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1)], dim=1)  # Shape: (batch, 32*31 + 32*31) = (batch, 1984)\n",
        "        return self.fc(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaKZfQlxxkXY"
      },
      "outputs": [],
      "source": [
        "class PCNN_3Branch(nn.Module):\n",
        "    def __init__(self, input_channels=16, num_classes=2, input_length=80):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=20, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=10, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            out3 = self.branch3(example_input)\n",
        "            out4 = self.branch4(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel() + out3.numel() + out4.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        out4 = self.branch4(x)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1), out3.flatten(1), out4.flatten(1)], dim=1)\n",
        "        return F.softmax(self.fc(combined), dim = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSeqtRO_t8hk"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "class EEGAugmenter:\n",
        "    def __call__(self, sample):\n",
        "        if np.random.rand() > 0.5:\n",
        "            # Add Gaussian noise\n",
        "            noise = torch.randn_like(sample) * 0.01\n",
        "            sample += noise\n",
        "        if np.random.rand() > 0.5:\n",
        "            # Random shift (up to 5 timesteps)\n",
        "            shift = np.random.randint(-5, 5)\n",
        "            sample = torch.roll(sample, shifts=shift, dims=-1)\n",
        "        return sample\n",
        "\n",
        "keep_channels = [2, 3, 10, 11, 12, 13, 14, 15]\n",
        "augmented_data = augmented_data[:, keep_channels, :]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(augmented_data, dtype=torch.float32)\n",
        "y = torch.tensor(augmented_labels, dtype=torch.long)  # Ensure labels are integers\n",
        "\n",
        "# Split into train/validation (80/20)\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 24  # Adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=EEGAugmenter())\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=EEGAugmenter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2V8EYSIuCFD",
        "outputId": "412d23d9-a4ff-4922-96df-b27e3ebd5e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
            "  return F.conv1d(\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PCNN_3Branch(input_channels=16, num_classes=2).to(device)  # Adjust num_classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS5ZPBVOIIsK"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Check for NaNs/Infs in data and labels\n",
        "assert not torch.isnan(X).any(), \"Input data contains NaNs!\"\n",
        "assert not torch.isinf(X).any(), \"Input data contains Infs!\"\n",
        "assert torch.all(y >= 0) and torch.all(y < 2), \"Invalid labels!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "68mtW1CIuD73"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import matplotlib.pyplot as plt\n",
        "num_epochs = 30  # Adjust based on early stopping\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "max_error = 0\n",
        "errors = []\n",
        "neutral = 0\n",
        "total_samples = 0\n",
        "#model.load_state_dict(torch.load('best_modelLowError.pt'))\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss, correct_train, total_train = 0, 0, 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # Add these checks\n",
        "        if torch.isnan(outputs).any():\n",
        "            print(\"NaN in model outputs!\")\n",
        "            break\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        epoch_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += batch_y.size(0)\n",
        "        correct_train += (predicted == batch_y).sum().item()\n",
        "\n",
        "   # Validation phase\n",
        "    model.eval()\n",
        "\n",
        "    epoch_val_loss, correct_val, total_val = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            total_samples += batch_y.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += batch_y.size(0)\n",
        "            correct_val += (predicted == batch_y).sum().item()\n",
        "\n",
        "            if (epoch == num_epochs -1):\n",
        "              total_samples += batch_y.size(0)\n",
        "              for i in range(len(predicted)):\n",
        "                if (outputs.data[i][1] < 0.98) and (outputs.data[i][0] < 0.98):\n",
        "                  neutral += 1\n",
        "\n",
        "              for i in range(len(predicted)):\n",
        "                if predicted[i] != batch_y[i]:\n",
        "                  errors.append(outputs.data[i][0 if batch_y[i] == 1 else 1].cpu().numpy())\n",
        "                  if outputs.data[i][0 if batch_y[i] == 1 else 1] < 0.5: #model is wrong\n",
        "                    print(f\"Weird things are happening: {outputs.data[i][0 if batch_y[i] == 1 else 1]}, {outputs.data[i][1 if batch_y[i] == 1 else 0]}\")\n",
        "    #Compute epoch metrics\n",
        "    train_loss = epoch_train_loss / len(train_loader)\n",
        "    val_loss = epoch_val_loss / len(val_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "        print(\"----------------------------------\")\n",
        "print(f\"Max Error: {max_error}\")\n",
        "plt.hist(errors, bins = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM0uV1pllXGF"
      },
      "outputs": [],
      "source": [
        "errors.sort()\n",
        "print(f\"90% Cutoff: {errors[int(len(errors)*0.9)]}\")\n",
        "print(f\"95% Cutoff: {errors[int(len(errors)*0.95)]}\")\n",
        "print(f\"99% Cutoff: {errors[int(len(errors)*0.98)]}\")\n",
        "error_total = len(errors)\n",
        "print(f\"{total_samples}\")\n",
        "print(f\"{neutral}\")\n",
        "labels = [\"non-neutral\", \"neutral\"]\n",
        "sizes = [total_samples - neutral, neutral]\n",
        "plt.pie(sizes, labels=labels)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "#torch.save(model.state_dict(), 'best_modelLowError2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2Y6RIXzknu2X"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMT6Q2xjuGdD"
      },
      "outputs": [],
      "source": [
        "#Try using optuna to optimize: Channel count, channel filter count, dropout probability, kernel length, linear layer depth, linear layer size\n",
        "#Not trying different pooling sizes, can attempt later\n",
        "batch_size = 24\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, trial, parallel_channels,\n",
        "                 kernel_size,\n",
        "                 num_conv_layers, output_channels,\n",
        "                 drop, linear_layers,\n",
        "                 linear_depth, num_classes,\n",
        "                 input_channels, pool_size, activation):\n",
        "        \"\"\"Parameters:\n",
        "            - trial (optuna.trial._trial.Trial): Optuna trial\n",
        "            - kernel_size (list):                Size of the kernels\n",
        "            - parallel_channels (int):           Number of parallel channels\n",
        "            - num_conv_layers (int):             Number of convolutional layers, same for all parallel channels\n",
        "            - output_channels (list):            Output channels for each convolutional layer\n",
        "            - drop (int):    Dropout probability for convolutional layers\n",
        "            - linear_layers (int):               Number of linear layers, needs to start at 0 actually, not 1\n",
        "            - linear_depth (list):               Depth of each linear layer\n",
        "            - num_classes (int):                 Number of output classes\n",
        "            - input_channels (int):              Number of input channels\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()                                                     # Initialize parent class\n",
        "        in_size = 80\n",
        "        self.parallels = nn.ModuleList()                                                     # List with the parallel channels\n",
        "        j = 0\n",
        "        self.out_sizes = []\n",
        "        self.norms = None\n",
        "        for i in range(parallel_channels):\n",
        "          # Define the convolutional layers for each parallel channel\n",
        "          self.convs = nn.ModuleList([nn.Conv1d(input_channels, output_channels[j], kernel_size=kernel_size[j], padding='same')])  # List with the Conv layers\n",
        "          out_size = in_size - kernel_size[j] + 1                                            # Size of the output kernel\n",
        "          j += 1\n",
        "          out_size = int(out_size / 2)                                                      # Size after pooling\n",
        "          for k in range(1, num_conv_layers):\n",
        "              self.convs.append(nn.Conv1d(in_channels=output_channels[j-1], out_channels=output_channels[j], kernel_size=kernel_size[j]))\n",
        "              out_size = out_size - kernel_size[j] + 1                                       # Size of the output kernel\n",
        "              out_size = int(out_size/2)                                               # Size after pooling\n",
        "              j += 1\n",
        "          self.out_sizes.append(out_size)\n",
        "          self.parallels.append(self.convs)                                               #Add whole parallel channel to parallels list\n",
        "        self.conv_drop = nn.Dropout1d(p=drop)                                             #convolutional dropout\n",
        "\n",
        "        self.drop = drop\n",
        "\n",
        "\n",
        "        # Dynamically calculate FC input dimension in forward pass\n",
        "        self.fc_input_dim = None # Initialize to None\n",
        "\n",
        "        self.layers = []\n",
        "        self.linear_layers = linear_layers # Store linear_layers\n",
        "        self.linear_depth = linear_depth # Store linear_depth\n",
        "        self.num_classes = num_classes # Store num_classes\n",
        "\n",
        "        self.pool_size = pool_size # store attempted pooling size\n",
        "        self.activation = activation # store activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for j, parallel_i in enumerate(self.parallels):\n",
        "            branch_output = x\n",
        "            for i, conv_i in enumerate(self.parallels[j]):\n",
        "                branch_output = conv_i(branch_output)\n",
        "                #self.norms = nn.BatchNorm1d(branch_output.shape[-2])\n",
        "                match self.activation:\n",
        "                  case \"relu\":\n",
        "                    branch_output = F.relu(F.max_pool1d(self.conv_drop((branch_output)), self.pool_size))\n",
        "                  case \"tanh\":\n",
        "                    branch_output = F.tanh(F.max_pool1d(self.conv_drop((branch_output)), self.pool_size))\n",
        "                  case \"leaky_relu\":\n",
        "                    branch_output = F.leaky_relu(F.max_pool1d(self.conv_drop((branch_output)), self.pool_size))\n",
        "            outputs.append(branch_output.flatten(1))\n",
        "\n",
        "        total_out_features = sum(out.shape[1] for out in outputs)  # Calculate total output features\n",
        "        if self.fc_input_dim is None or self.fc_input_dim != total_out_features:\n",
        "            self.fc_input_dim = total_out_features\n",
        "            # Re-create fully connected layers based on calculated input dimension\n",
        "            self.layers = []\n",
        "            out_feature = self.fc_input_dim\n",
        "            for i in range(self.linear_layers):\n",
        "                self.layers.append(nn.Linear(out_feature, self.linear_depth[i]))\n",
        "                self.layers.append(nn.Dropout(p = self.drop))\n",
        "                out_feature = self.linear_depth[i]\n",
        "            self.layers.append(nn.Linear(out_feature, self.num_classes))\n",
        "            self.fc = nn.Sequential(*self.layers).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate outputs from all branches along the channel dimension\n",
        "        x = torch.cat(outputs, dim = 1)\n",
        "        match self.activation:\n",
        "            case \"relu\":\n",
        "                x = F.relu(self.fc(x))\n",
        "            case \"tanh\":\n",
        "                x = F.tanh(self.fc(x))\n",
        "            case \"leaky_relu\":\n",
        "                x = F.relu(self.fc(x))                               #pass through all layers\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "def train(model, optimizer):\n",
        "    \"\"\"Trains the model.\n",
        "\n",
        "    Parameters:\n",
        "        - network (__main__.Net):              The CNN\n",
        "        - optimizer (torch.optim.<optimizer>): The optimizer for the CNN\n",
        "    \"\"\"\n",
        "    Epoch_Count = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()  # Set the module in training mode (only affects certain modules)\n",
        "    for batch_i, (data, target) in enumerate(train_loader):  # For each batch\n",
        "\n",
        "        if data.shape[0] != batch_size:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()                                 # Clear gradients\n",
        "        output = model(data.to(device))                     # Forward propagation\n",
        "        loss = criterion(output, target.to(device))          # Compute loss (negative log likelihood: −log(y))\n",
        "        loss.backward()                                       # Compute gradients\n",
        "        optimizer.step()                                      # Update weights\n",
        "    Epoch_Count += 1\n",
        "def test(model):\n",
        "    \"\"\"Tests the model.\n",
        "\n",
        "    Parameters:\n",
        "        - network (__main__.Net): The CNN\n",
        "\n",
        "    Returns:\n",
        "        - accuracy_test (torch.Tensor): The test accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()        # Set the module in evaluation mode (only affects certain modules)\n",
        "    total_val = 1\n",
        "    correct_val = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation (when you are sure that you will not call Tensor.backward())\n",
        "      for batch, (data, target) in enumerate(val_loader):  # For each batch\n",
        "        if data.shape[0] != batch_size:\n",
        "          break\n",
        "        output = model(data.to(device))               # Forward propagation\n",
        "        argmax = []\n",
        "        for element in output:\n",
        "          if element[0] > element[1]: argmax.append(0)\n",
        "          else: argmax.append(1)\n",
        "        outputs = torch.tensor(argmax).to(device)\n",
        "        predicted = outputs\n",
        "        total_val += target.size(0)\n",
        "        correct_val += (predicted == target.to(device)).sum().item()\n",
        "    accuracy_test = correct_val / total_val\n",
        "    return accuracy_test\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Objective function to be optimized by Optuna.\n",
        "\n",
        "    Hyperparameters chosen to be optimized: optimizer, learning rate,\n",
        "    dropout values, number of convolutional layers, number of filters of\n",
        "    convolutional layers, number of neurons of fully connected layers.\n",
        "\n",
        "    Inputs:\n",
        "        - trial (optuna.trial._trial.Trial): Optuna trial\n",
        "    Returns:\n",
        "        - accuracy(torch.Tensor): The test accuracy. Parameter to be maximized.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define range of values to be tested for the hyperparameters\n",
        "    # first layer input length = 80\n",
        "    # second layer input length = 32\n",
        "    # third layer input length = 8\n",
        "\n",
        "    parallel_channels = trial.suggest_int(\"parallel_channels\", 1, 6)  # Number of parallel channels\n",
        "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)  # Number of convolutional layers\n",
        "    kernel_size = [0] * parallel_channels * num_conv_layers\n",
        "    output_channels = [0] * parallel_channels * num_conv_layers\n",
        "    for i in range(parallel_channels * num_conv_layers):\n",
        "      if (i + 1) % num_conv_layers == 0:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 2, 8)\n",
        "      else:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 3, 16)\n",
        "    for i in range(num_conv_layers * parallel_channels):\n",
        "      output_channels[i] = trial.suggest_int(\"output_channels_\"+str(i), 4, 32, 4)\n",
        "    drop = trial.suggest_float(\"drop\", 0.2, 0.5)  # Dropout probability\n",
        "    linear_layers = trial.suggest_int(\"linear_layers\", 0, 3)  # Number of linear layers\n",
        "    linear_depth = [int(trial.suggest_discrete_uniform(\"linear_depth_\"+str(i), 10, 400, 10))\n",
        "                    for i in range(linear_layers)]  # Depth of each linear layer\n",
        "\n",
        "    #Try using optuna to optimize: Channel count, channel filter count, dropout probability, kernel length, linear layer depth, linear layer size\n",
        "    #Now adding extra parameters to attempt: pooling size, activation,\n",
        "\n",
        "    pool_size = trial.suggest_int(\"pooling_size\", 2, 4)\n",
        "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\", \"leaky_relu\"])\n",
        "\n",
        "\n",
        "    parallel_channels =  6\n",
        "    num_conv_layers =  2\n",
        "    #kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "    #output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "    kernel_size = [0] * parallel_channels * num_conv_layers\n",
        "    output_channels = [0] * parallel_channels * num_conv_layers\n",
        "    for i in range(parallel_channels * num_conv_layers):\n",
        "      if (i + 1) % num_conv_layers == 0:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 2, 8)\n",
        "      else:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 3, 16)\n",
        "    for i in range(num_conv_layers * parallel_channels):\n",
        "      output_channels[i] = trial.suggest_int(\"output_channels_\"+str(i), 4, 32, 4)\n",
        "    drop = 0.2915637951496211\n",
        "    linear_layers = 0\n",
        "    linear_depth = 1\n",
        "    lr =  0.008900505464977875\n",
        "    num_classes = 2\n",
        "\n",
        "    # Generate the model\n",
        "    model = Net(trial, parallel_channels,\n",
        "                 kernel_size,\n",
        "                 num_conv_layers, output_channels,\n",
        "                 drop, linear_layers,\n",
        "                 linear_depth, num_classes, 16, pool_size, activation).to(device)\n",
        "\n",
        "    # Generate the optimizers\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])  # Optimizers\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)                                 # Learning rates\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    lr =  0.008900505464977875\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "    num_epochs = 42\n",
        "\n",
        "    # Training of the model\n",
        "    max_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train(model, optimizer)  # Train the model\n",
        "        accuracy = test(model)   # Evaluate the model\n",
        "        if accuracy > max_acc:\n",
        "          max_acc = accuracy\n",
        "          torch.save(model.state_dict(), 'best_model_lr.pt')\n",
        "\n",
        "        # For pruning (stops trial early if not promising)\n",
        "        trial.report(accuracy, epoch)\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Optimization study for a PyTorch CNN with Optuna\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Use cuda if available for faster computations\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Parameters ----------------------------------------------------------\n",
        "\n",
        "    batch_size_train = 24               # Batch size for training data\n",
        "    batch_size_test = 24               # Batch size for testing data\n",
        "    number_of_trials = 500                # Number of Optuna trials\n",
        "    limit_obs = False                      # Limit number of observations for faster computation\n",
        "\n",
        "    # *** Note: For more accurate results, do not limit the observations.\n",
        "    #           If not limited, however, it might take a very long time to run.\n",
        "    #           Another option is to limit the number of epochs. ***\n",
        "\n",
        "    if limit_obs:  # Limit number of observations\n",
        "        number_of_train_examples = 500 * batch_size_train  # Max train observations\n",
        "        number_of_test_examples = 5 * batch_size_test      # Max test observations\n",
        "    else:\n",
        "        number_of_train_examples = 60000                   # Max train observations\n",
        "        number_of_test_examples = 10000                    # Max test observations\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Make runs repeatable\n",
        "    random_seed = 1\n",
        "    torch.backends.cudnn.enabled = False  # Disable cuDNN use of nondeterministic algorithms\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Create an Optuna study to maximize test accuracy\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=number_of_trials)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Results\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Find number of pruned and completed trials\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    # Display the study statistics\n",
        "    print(\"\\nStudy statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    trial = study.best_trial\n",
        "    print(\"Best trial:\")\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    # Save results to csv file\n",
        "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
        "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
        "    df = df.drop('state', axis=1)                 # Exclude state column\n",
        "    df = df.sort_values('value')                  # Sort based on accuracy\n",
        "    df.to_csv('optuna_results.csv', index=False)  # Save to csv file\n",
        "\n",
        "    # Display results in a dataframe\n",
        "    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n",
        "\n",
        "    # Find the most important hyperparameters\n",
        "    most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
        "\n",
        "    # Display the most important hyperparameters\n",
        "    print('\\nMost important hyperparameters:')\n",
        "    for key, value in most_important_parameters.items():\n",
        "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ipAA3mO_t9x-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "5eccf78a-c63e-45cd-ff90-7b82dd9c813c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d08c75824980>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlinear_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.008900505464977875\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m  \u001b[0;31m# Adjust based on early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# kernel_size_0': 4, 'kernel_size_1': 7, 'kernel_size_2': 6,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "#test new model\n",
        "# @title\n",
        "#Best acc = 83.4% (42 epochs)\n",
        "parallel_channels =  6\n",
        "num_conv_layers =  2\n",
        "kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "drop = 0.2915637951496211\n",
        "linear_layers = 0\n",
        "lr =  0.008900505464977875\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 42  # Adjust based on early stopping\n",
        "# kernel_size_0': 4, 'kernel_size_1': 7, 'kernel_size_2': 6,\n",
        "# 'kernel_size_3': 5, 'kernel_size_4': 6, 'kernel_size_5': 4,\n",
        "# 'kernel_size_6': 10, 'kernel_size_7': 6, 'kernel_size_8': 13,\n",
        "# 'kernel_size_9': 4, 'kernel_size_10': 15, 'kernel_size_11': 7,\n",
        "# 'output_channels_0': 8, 'output_channels_1': 16,\n",
        "# 'output_channels_2': 4, 'output_channels_3': 4,\n",
        "# 'output_channels_4': 4, 'output_channels_5': 28,\n",
        "# 'output_channels_6': 28, 'output_channels_7': 4,\n",
        "# 'output_channels_8': 8, 'output_channels_9': 8,\n",
        "# 'output_channels_10': 8, 'output_channels_11': 24\n",
        "\n",
        "class TestNet(nn.Module):\n",
        "  def __init__(self, input_channels, input_length, num_classes, drop):\n",
        "        super(TestNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.input_length = input_length\n",
        "        self.num_classes = num_classes\n",
        "        self.pool_size = 2\n",
        "\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 12, kernel_size=4, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(12, 32, kernel_size=7, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 24, kernel_size=6, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(24, 12, kernel_size=5, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 8, kernel_size=6, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(8, 8, kernel_size=4, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch5 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=13, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(16, 8, kernel_size=4, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 20, kernel_size=10, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(20, 4, kernel_size=6, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch6 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 12, kernel_size=15, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(self.pool_size),\n",
        "            nn.Conv1d(12, 4, kernel_size=7, padding='same'),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            out3 = self.branch3(example_input)\n",
        "            out4 = self.branch4(example_input)\n",
        "            out5 = self.branch5(example_input)\n",
        "            out6 = self.branch6(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel() + out3.numel() + out4.numel() + out5.numel() + out6.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, num_classes),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        out4 = self.branch4(x)\n",
        "        out5 = self.branch5(x)\n",
        "        out6 = self.branch6(x)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1), out3.flatten(1), out4.flatten(1), out5.flatten(1), out6.flatten(1)], dim=1)\n",
        "        return F.softmax(self.fc(combined), dim = -1)\n",
        "\n",
        "model = TestNet(8, 80, 2, 0.2915637951496211).to(device)\n",
        "#model.load_state_dict(torch.load('best_model_85_71.pt'))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.008900505464977875)\n",
        "model.eval()\n",
        "\n",
        "parallel_channels =  6\n",
        "num_conv_layers =  2\n",
        "kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "drop = 0.2915637951496211\n",
        "linear_layers = 0\n",
        "lr =  0.008900505464977875\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "num_epochs = 100\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "current_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    #Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss, correct_train, total_train = 0, 0, 0\n",
        "    for batch, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        outputs = model(data)\n",
        "\n",
        "        # Add these checks\n",
        "        if torch.isnan(outputs).any():\n",
        "            print(\"NaN in model outputs!\")\n",
        "            break\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        epoch_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += targets.size(0)\n",
        "        correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    epoch_val_loss, correct_val, total_val = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, targets) in enumerate(val_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += targets.size(0)\n",
        "            correct_val += (predicted == targets).sum().item()\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    train_loss = epoch_train_loss / len(train_loader)\n",
        "    val_loss = epoch_val_loss / len(val_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    if (val_acc > current_acc):\n",
        "      current_acc = val_acc;\n",
        "      torch.save(model.state_dict(), 'best_model_8_channels.pt')\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "    print(\"----------------------------------\")\n",
        "print(f\"Final val acc: {current_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WjlQFJf3PER"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}