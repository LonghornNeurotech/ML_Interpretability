{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqcjUC3bOKhu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import scipy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader as DL\n",
        "from torch.utils.data import TensorDataset as TData\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import pickle\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4PgfyyOjP6",
        "outputId": "d50de302-f9fc-41b6-fd0f-2f4916f4475b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "WYjr4RAoVUGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = '/content/drive/MyDrive/LHNTData'"
      ],
      "metadata": {
        "id": "ZLZKhGs5vBCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSYL5T_lOKhw"
      },
      "outputs": [],
      "source": [
        "# Base folder containing the files\n",
        "# right_session_folder = os.path.join(root, \"alan_f_right/session_1\")\n",
        "\n",
        "# Initialize a dictionary to store the loaded signals\n",
        "right_eeg_data = {}\n",
        "\n",
        "# Iterate through files labeled in increments of two\n",
        "for j in range(1, 4):\n",
        "  right_session_folder = os.path.join(root, f\"morgan_dye_Session{j}\")\n",
        "  for i in range(2, 21, 2):  # Adjust the range as needed\n",
        "\n",
        "      file_path = os.path.join(right_session_folder, f\"right_16.pkl\")\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "          with open(file_path, \"rb\") as file:\n",
        "              data = pickle.load(file)\n",
        "              right_eeg_data[f\"right_{j}_{i}\"] = data  # Store the data with the corresponding label\n",
        "      else:\n",
        "          print(f\"File not found: {file_path}\")\n",
        "          break  # Stop if the file sequence ends\n",
        "\n",
        "left_eeg_data = {}\n",
        "\n",
        "for j in range(1, 4):\n",
        "  left_session_folder = os.path.join(root, f\"morgan_dye_Session{j}\")\n",
        "  # Iterate through files labeled in increments of two\n",
        "  for i in range(1, 21, 2):  # Adjust the range as needed\n",
        "      file_path = os.path.join(left_session_folder, f\"left_19.pkl\")\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "          with open(file_path, \"rb\") as file:\n",
        "              data = pickle.load(file)\n",
        "              left_eeg_data[f\"left_{j}_{i}\"] = data  # Store the data with the corresponding label\n",
        "      else:\n",
        "          print(f\"File not found: {file_path}\")\n",
        "          break  # Stop if the file sequence ends\n",
        "\n",
        "\n",
        "\n",
        "right_signal_data = {label: signal[0] for label, signal in right_eeg_data.items()}\n",
        "right_metadata = {label: signal[1] for label, signal in right_eeg_data.items()}\n",
        "\n",
        "left_signal_data = {label: signal[0] for label, signal in left_eeg_data.items()}\n",
        "metadata = {label: signal[1] for label, signal in left_eeg_data.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def baseline_correction(signal):\n",
        "    \"\"\"\n",
        "    Removes baseline offset by subtracting the mean of each channel.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Baseline-corrected signal.\n",
        "    \"\"\"\n",
        "    return signal - np.mean(signal, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def bandpass_filter(signal, lowcut=13, highcut=30, fs=125, order=4):\n",
        "    \"\"\"\n",
        "    Band-pass filters the signal for the specified frequency range.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "        lowcut (float): Lower cutoff frequency (Hz).\n",
        "        highcut (float): Upper cutoff frequency (Hz).\n",
        "        fs (float): Sampling rate (Hz).\n",
        "        order (int): Order of the filter.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Band-pass filtered signal.\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype=\"band\")\n",
        "    return filtfilt(b, a, signal, axis=1)\n",
        "\n",
        "def normalize_signal(signal):\n",
        "    \"\"\"\n",
        "    Normalizes the signal for each channel (z-score normalization).\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized signal.\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(signal.T).T  # Transpose to normalize each channel\n",
        "\n",
        "def preprocess_eeg(signal, fs=256):\n",
        "    \"\"\"\n",
        "    Preprocess EEG signal with baseline correction, band-pass filtering, and normalization.\n",
        "\n",
        "    Parameters:\n",
        "        signal (np.ndarray): EEG signal with shape (channels, samples).\n",
        "        fs (float): Sampling rate (Hz).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Preprocessed EEG signal.\n",
        "    \"\"\"\n",
        "    # Step 1: Baseline Correction\n",
        "    signal_corrected = baseline_correction(signal)\n",
        "\n",
        "    # Step 2: Band-Pass Filter (Beta Frequencies)\n",
        "    signal_filtered = bandpass_filter(signal_corrected, lowcut=4, highcut=40, fs=fs)\n",
        "\n",
        "    # Step 3: Normalization\n",
        "    signal_normalized = normalize_signal(signal_filtered)\n",
        "\n",
        "    return signal_normalized\n",
        "\n",
        "def sliding_window_augmentation(data, labels, window_size=125, stride=80):\n",
        "    \"\"\"\n",
        "    Apply sliding window to already EMD-processed data\n",
        "\n",
        "    Args:\n",
        "    data: numpy array of shape (n_samples, n_channels*n_imfs, time_steps)\n",
        "    labels: numpy array of shape (n_samples,)\n",
        "    window_size: size of sliding window\n",
        "    stride: step size for sliding window\n",
        "\n",
        "    Returns:\n",
        "    augmented_data, augmented_labels\n",
        "    \"\"\"\n",
        "    n_samples, n_features, time_steps = data.shape\n",
        "    n_windows = (time_steps - window_size) // stride + 1\n",
        "\n",
        "    augmented_data = np.zeros((n_samples * n_windows, n_features, window_size))\n",
        "    augmented_labels = np.zeros(n_samples * n_windows, dtype=labels.dtype)\n",
        "\n",
        "    idx = 0\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_windows):\n",
        "            start = j * stride\n",
        "            end = start + window_size\n",
        "            augmented_data[idx] = data[i, :, start:end]\n",
        "            augmented_labels[idx] = labels[i]\n",
        "            idx += 1\n",
        "\n",
        "    return augmented_data, augmented_labels"
      ],
      "metadata": {
        "id": "qicPbkMivQuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess each signal in the dictionary\n",
        "right_preprocessed_signals = {label: preprocess_eeg(signal, fs=256) for label, signal in right_signal_data.items()}\n",
        "\n",
        "# Inspect the shapes of the processed signals\n",
        "for label, signal in right_preprocessed_signals.items():\n",
        "    print(f\"{label}: RIGHT Processed Signal Shape = {signal.shape}\")\n",
        "\n",
        "\n",
        "# Preprocess each signal in the dictionary\n",
        "left_preprocessed_signals = {label: preprocess_eeg(signal, fs=256) for label, signal in left_signal_data.items()}\n",
        "\n",
        "# Inspect the shapes of the processed signals\n",
        "for label, signal in left_preprocessed_signals.items():\n",
        "    print(f\"{label}: LEFT Processed Signal Shape = {signal.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyOEe9eKvTMM",
        "outputId": "a6c19ce2-4d1e-4d74-b69a-feb0d181f94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right_1_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_1_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_2_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_2: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_4: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_6: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_8: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_10: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_12: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_14: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_16: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_18: RIGHT Processed Signal Shape = (16, 875)\n",
            "right_3_20: RIGHT Processed Signal Shape = (16, 875)\n",
            "left_1_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_1_19: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_2_19: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_1: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_3: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_5: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_7: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_9: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_11: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_13: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_15: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_17: LEFT Processed Signal Shape = (16, 875)\n",
            "left_3_19: LEFT Processed Signal Shape = (16, 875)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "left_labels = np.array([0 for _ in range(len(left_preprocessed_signals))])\n",
        "right_labels = np.array([1 for _ in range(len(right_preprocessed_signals))])\n",
        "\n",
        "left, left_labels = sliding_window_augmentation(np.array(list(left_preprocessed_signals.values())), np.array(left_labels), window_size=80, stride=60)\n",
        "right, right_labels = sliding_window_augmentation(np.array(list(right_preprocessed_signals.values())), np.array(right_labels), window_size=80, stride=60)"
      ],
      "metadata": {
        "id": "2b5NTIexvfJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation Functions"
      ],
      "metadata": {
        "id": "eaol39KVVP36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model things\n"
      ],
      "metadata": {
        "id": "bdJQRTVLv0IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this if no EMD!\n",
        "augmented_data = np.concatenate((left, right))\n",
        "augmented_labels = np.concatenate((left_labels, right_labels))\n"
      ],
      "metadata": {
        "id": "NkbYjcWNwUGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMD_PCNN(nn.Module):\n",
        "    def __init__(self, input_channels=16, num_classes=2, input_length=80):\n",
        "        super().__init__()\n",
        "\n",
        "        # Branch 1 (input_length = 125)\n",
        "        # Branch 1 (input_length = 125)\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=20, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),  # Output length: 125 // 2 = 62\n",
        "            # nn.Conv1d(16, 32, kernel_size=10, padding='same'),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.MaxPool1d(2)  # Output length: 62 // 2 = 31\n",
        "        )\n",
        "\n",
        "        # Branch 2 (input_length = 125)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=10, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),  # Output length: 125 // 2 = 62\n",
        "            # nn.Conv1d(16, 32, kernel_size=5, padding='same'),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # nn.MaxPool1d(2)  # Output length: 62 // 2 = 31\n",
        "        )\n",
        "\n",
        "        # Calculate FC input dimension dynamically\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1(x)  # Shape: (batch, 32, 31)\n",
        "        out2 = self.branch2(x)  # Shape: (batch, 32, 31)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1)], dim=1)  # Shape: (batch, 32*31 + 32*31) = (batch, 1984)\n",
        "        return self.fc(combined)"
      ],
      "metadata": {
        "id": "qQZE4KRzso-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCNN_3Branch(nn.Module):\n",
        "    def __init__(self, input_channels=16, num_classes=2, input_length=80):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=20, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=10, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=5, padding='same'),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            out3 = self.branch3(example_input)\n",
        "            out4 = self.branch4(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel() + out3.numel() + out4.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        out4 = self.branch4(x)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1), out3.flatten(1), out4.flatten(1)], dim=1)\n",
        "        return F.softmax(self.fc(combined), dim = -1)"
      ],
      "metadata": {
        "id": "kaKZfQlxxkXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "class EEGAugmenter:\n",
        "    def __call__(self, sample):\n",
        "        if np.random.rand() > 0.5:\n",
        "            # Add Gaussian noise\n",
        "            noise = torch.randn_like(sample) * 0.01\n",
        "            sample += noise\n",
        "        if np.random.rand() > 0.5:\n",
        "            # Random shift (up to 5 timesteps)\n",
        "            shift = np.random.randint(-5, 5)\n",
        "            sample = torch.roll(sample, shifts=shift, dims=-1)\n",
        "        return sample\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(augmented_data, dtype=torch.float32)\n",
        "y = torch.tensor(augmented_labels, dtype=torch.long)  # Ensure labels are integers\n",
        "\n",
        "# Split into train/validation (80/20)\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 24  # Adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=EEGAugmenter())\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=EEGAugmenter())"
      ],
      "metadata": {
        "id": "PSeqtRO_t8hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PCNN_3Branch(input_channels=16, num_classes=2).to(device)  # Adjust num_classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "F2V8EYSIuCFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ebdd2c-fe7f-43a2-df84-0323e40cedd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
            "  return F.conv1d(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Check for NaNs/Infs in data and labels\n",
        "assert not torch.isnan(X).any(), \"Input data contains NaNs!\"\n",
        "assert not torch.isinf(X).any(), \"Input data contains Infs!\"\n",
        "assert torch.all(y >= 0) and torch.all(y < 2), \"Invalid labels!\""
      ],
      "metadata": {
        "id": "nS5ZPBVOIIsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import matplotlib.pyplot as plt\n",
        "num_epochs = 30  # Adjust based on early stopping\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "max_error = 0\n",
        "errors = []\n",
        "neutral = 0\n",
        "total_samples = 0\n",
        "model.load_state_dict(torch.load('best_modelLowError.pt'))\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_loss, correct_train, total_train = 0, 0, 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # Add these checks\n",
        "        if torch.isnan(outputs).any():\n",
        "            print(\"NaN in model outputs!\")\n",
        "            break\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        epoch_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += batch_y.size(0)\n",
        "        correct_train += (predicted == batch_y).sum().item()\n",
        "\n",
        "   # Validation phase\n",
        "    model.eval()\n",
        "\n",
        "    epoch_val_loss, correct_val, total_val = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            total_samples += batch_y.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += batch_y.size(0)\n",
        "            correct_val += (predicted == batch_y).sum().item()\n",
        "\n",
        "            if (epoch == num_epochs -1):\n",
        "              total_samples += batch_y.size(0)\n",
        "              for i in range(len(predicted)):\n",
        "                if (outputs.data[i][1] < 0.98) and (outputs.data[i][0] < 0.98):\n",
        "                  neutral += 1\n",
        "\n",
        "              for i in range(len(predicted)):\n",
        "                if predicted[i] != batch_y[i]:\n",
        "                  errors.append(outputs.data[i][0 if batch_y[i] == 1 else 1].cpu().numpy())\n",
        "                  if outputs.data[i][0 if batch_y[i] == 1 else 1] < 0.5: #model is wrong\n",
        "                    print(f\"Weird things are happening: {outputs.data[i][0 if batch_y[i] == 1 else 1]}, {outputs.data[i][1 if batch_y[i] == 1 else 0]}\")\n",
        "    #Compute epoch metrics\n",
        "    train_loss = epoch_train_loss / len(train_loader)\n",
        "    val_loss = epoch_val_loss / len(val_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "        print(\"----------------------------------\")\n",
        "print(f\"Max Error: {max_error}\")\n",
        "plt.hist(errors, bins = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "68mtW1CIuD73",
        "outputId": "573a7494-9d32-48ba-fd25-12aa9c663f50",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Train Loss: 0.4470 | Val Loss: 0.0000\n",
            "Train Acc: 83.84% | Val Acc: 80.56%\n",
            "----------------------------------\n",
            "Epoch 3/30\n",
            "Train Loss: 0.4497 | Val Loss: 0.0000\n",
            "Train Acc: 85.20% | Val Acc: 78.97%\n",
            "----------------------------------\n",
            "Epoch 5/30\n",
            "Train Loss: 0.4433 | Val Loss: 0.0000\n",
            "Train Acc: 85.03% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 7/30\n",
            "Train Loss: 0.4483 | Val Loss: 0.0000\n",
            "Train Acc: 84.18% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 9/30\n",
            "Train Loss: 0.4456 | Val Loss: 0.0000\n",
            "Train Acc: 84.86% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 11/30\n",
            "Train Loss: 0.4501 | Val Loss: 0.0000\n",
            "Train Acc: 82.82% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 13/30\n",
            "Train Loss: 0.4443 | Val Loss: 0.0000\n",
            "Train Acc: 84.01% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 15/30\n",
            "Train Loss: 0.4431 | Val Loss: 0.0000\n",
            "Train Acc: 83.84% | Val Acc: 79.76%\n",
            "----------------------------------\n",
            "Epoch 17/30\n",
            "Train Loss: 0.4436 | Val Loss: 0.0000\n",
            "Train Acc: 85.71% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 19/30\n",
            "Train Loss: 0.4390 | Val Loss: 0.0000\n",
            "Train Acc: 85.54% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 21/30\n",
            "Train Loss: 0.4424 | Val Loss: 0.0000\n",
            "Train Acc: 84.86% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 23/30\n",
            "Train Loss: 0.4476 | Val Loss: 0.0000\n",
            "Train Acc: 83.67% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 25/30\n",
            "Train Loss: 0.4414 | Val Loss: 0.0000\n",
            "Train Acc: 86.05% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 27/30\n",
            "Train Loss: 0.4467 | Val Loss: 0.0000\n",
            "Train Acc: 85.20% | Val Acc: 78.57%\n",
            "----------------------------------\n",
            "Epoch 29/30\n",
            "Train Loss: 0.4456 | Val Loss: 0.0000\n",
            "Train Acc: 85.71% | Val Acc: 78.97%\n",
            "----------------------------------\n",
            "Max Error: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errors.sort()\n",
        "print(f\"90% Cutoff: {errors[int(len(errors)*0.9)]}\")\n",
        "print(f\"95% Cutoff: {errors[int(len(errors)*0.95)]}\")\n",
        "print(f\"99% Cutoff: {errors[int(len(errors)*0.98)]}\")\n",
        "error_total = len(errors)\n",
        "print(f\"{total_samples}\")\n",
        "print(f\"{neutral}\")\n",
        "labels = [\"non-neutral\", \"neutral\"]\n",
        "sizes = [total_samples - neutral, neutral]\n",
        "plt.pie(sizes, labels=labels)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "#torch.save(model.state_dict(), 'best_modelLowError2.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "vM0uV1pllXGF",
        "outputId": "7c52a893-d0cd-42ac-a9e4-34125908d2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90% Cutoff: 0.8181471228599548\n",
            "95% Cutoff: 0.9250420928001404\n",
            "99% Cutoff: 0.9250420928001404\n",
            "7812\n",
            "84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y6RIXzknu2X",
        "outputId": "74dbf4da-cbbb-4e25-bef4-4d31eca08819",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try using optuna to optimize: Channel count, channel filter count, dropout probability, kernel length, linear layer depth, linear layer size\n",
        "#Not trying different pooling sizes, can attempt later\n",
        "batch_size = 24\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, trial, parallel_channels,\n",
        "                 kernel_size,\n",
        "                 num_conv_layers, output_channels,\n",
        "                 drop, linear_layers,\n",
        "                 linear_depth, num_classes,\n",
        "                 input_channels):\n",
        "        \"\"\"Parameters:\n",
        "            - trial (optuna.trial._trial.Trial): Optuna trial\n",
        "            - kernel_size (list):                Size of the kernels\n",
        "            - parallel_channels (int):           Number of parallel channels\n",
        "            - num_conv_layers (int):             Number of convolutional layers, same for all parallel channels\n",
        "            - output_channels (list):            Output channels for each convolutional layer\n",
        "            - drop (int):    Dropout probability for convolutional layers\n",
        "            - linear_layers (int):               Number of linear layers, needs to start at 0 actually, not 1\n",
        "            - linear_depth (list):               Depth of each linear layer\n",
        "            - num_classes (int):                 Number of output classes\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()                                                     # Initialize parent class\n",
        "        in_size = 80\n",
        "        self.parallels = nn.ModuleList()                                                     # List with the parallel channels\n",
        "        j = 0\n",
        "        self.out_sizes = []\n",
        "        self.norms = None\n",
        "        for i in range(parallel_channels):\n",
        "          # Define the convolutional layers for each parallel channel\n",
        "          self.convs = nn.ModuleList([nn.Conv1d(input_channels, output_channels[j], kernel_size=kernel_size[j], padding='same')])  # List with the Conv layers\n",
        "          out_size = in_size - kernel_size[j] + 1                                            # Size of the output kernel\n",
        "          j += 1\n",
        "          out_size = int(out_size / 2)                                                      # Size after pooling\n",
        "          for k in range(1, num_conv_layers):\n",
        "              self.convs.append(nn.Conv1d(in_channels=output_channels[j-1], out_channels=output_channels[j], kernel_size=kernel_size[j]))\n",
        "              out_size = out_size - kernel_size[j] + 1                                       # Size of the output kernel\n",
        "              out_size = int(out_size/2)                                               # Size after pooling\n",
        "              j += 1\n",
        "          self.out_sizes.append(out_size)\n",
        "          self.parallels.append(self.convs)                                               #Add whole parallel channel to parallels list\n",
        "        self.conv_drop = nn.Dropout1d(p=drop)                                             #convolutional dropout\n",
        "\n",
        "        self.drop = drop\n",
        "\n",
        "\n",
        "        # Dynamically calculate FC input dimension in forward pass\n",
        "        self.fc_input_dim = None # Initialize to None\n",
        "\n",
        "        self.layers = []\n",
        "        self.linear_layers = linear_layers # Store linear_layers\n",
        "        self.linear_depth = linear_depth # Store linear_depth\n",
        "        self.num_classes = num_classes # Store num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for j, parallel_i in enumerate(self.parallels):\n",
        "            branch_output = x\n",
        "            for i, conv_i in enumerate(self.parallels[j]):\n",
        "                branch_output = conv_i(branch_output)\n",
        "                #self.norms = nn.BatchNorm1d(branch_output.shape[-2])\n",
        "                branch_output = F.relu(F.max_pool1d(self.conv_drop((branch_output)), 2))\n",
        "            outputs.append(branch_output.flatten(1))\n",
        "\n",
        "        total_out_features = sum(out.shape[1] for out in outputs)  # Calculate total output features\n",
        "        if self.fc_input_dim is None or self.fc_input_dim != total_out_features:\n",
        "            self.fc_input_dim = total_out_features\n",
        "            # Re-create fully connected layers based on calculated input dimension\n",
        "            self.layers = []\n",
        "            out_feature = self.fc_input_dim\n",
        "            for i in range(self.linear_layers):\n",
        "                self.layers.append(nn.Linear(out_feature, self.linear_depth[i]))\n",
        "                self.layers.append(nn.Dropout(p = self.drop))\n",
        "                out_feature = self.linear_depth[i]\n",
        "            self.layers.append(nn.Linear(out_feature, self.num_classes))\n",
        "            self.fc = nn.Sequential(*self.layers).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate outputs from all branches along the channel dimension\n",
        "        x = torch.cat(outputs, dim = 1)\n",
        "        x = F.relu(self.fc(x))                               #pass through all layers\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "def train(model, optimizer):\n",
        "    \"\"\"Trains the model.\n",
        "\n",
        "    Parameters:\n",
        "        - network (__main__.Net):              The CNN\n",
        "        - optimizer (torch.optim.<optimizer>): The optimizer for the CNN\n",
        "    \"\"\"\n",
        "    Epoch_Count = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()  # Set the module in training mode (only affects certain modules)\n",
        "    for batch_i, (data, target) in enumerate(train_loader):  # For each batch\n",
        "\n",
        "        if data.shape[0] != batch_size:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()                                 # Clear gradients\n",
        "        output = model(data.to(device))                     # Forward propagation\n",
        "        loss = criterion(output, target.to(device))          # Compute loss (negative log likelihood: log(y))\n",
        "        loss.backward()                                       # Compute gradients\n",
        "        optimizer.step()                                      # Update weights\n",
        "    Epoch_Count += 1\n",
        "def test(model):\n",
        "    \"\"\"Tests the model.\n",
        "\n",
        "    Parameters:\n",
        "        - network (__main__.Net): The CNN\n",
        "\n",
        "    Returns:\n",
        "        - accuracy_test (torch.Tensor): The test accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()        # Set the module in evaluation mode (only affects certain modules)\n",
        "    total_val = 1\n",
        "    correct_val = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation (when you are sure that you will not call Tensor.backward())\n",
        "      for batch, (data, target) in enumerate(val_loader):  # For each batch\n",
        "        if data.shape[0] != batch_size:\n",
        "          break\n",
        "        output = model(data.to(device))               # Forward propagation\n",
        "        argmax = []\n",
        "        for element in output:\n",
        "          if element[0] > element[1]: argmax.append(0)\n",
        "          else: argmax.append(1)\n",
        "        outputs = torch.tensor(argmax).to(device)\n",
        "        predicted = outputs\n",
        "        total_val += target.size(0)\n",
        "        correct_val += (predicted == target.to(device)).sum().item()\n",
        "    accuracy_test = correct_val / total_val\n",
        "    return accuracy_test\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Objective function to be optimized by Optuna.\n",
        "\n",
        "    Hyperparameters chosen to be optimized: optimizer, learning rate,\n",
        "    dropout values, number of convolutional layers, number of filters of\n",
        "    convolutional layers, number of neurons of fully connected layers.\n",
        "\n",
        "    Inputs:\n",
        "        - trial (optuna.trial._trial.Trial): Optuna trial\n",
        "    Returns:\n",
        "        - accuracy(torch.Tensor): The test accuracy. Parameter to be maximized.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define range of values to be tested for the hyperparameters\n",
        "    # first layer input length = 80\n",
        "    # second layer input length = 32\n",
        "    # third layer input length = 8\n",
        "\n",
        "    # parallel_channels = trial.suggest_int(\"parallel_channels\", 1, 6)  # Number of parallel channels\n",
        "    # num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)  # Number of convolutional layers\n",
        "    # kernel_size = [0] * parallel_channels * num_conv_layers\n",
        "    # output_channels = [0] * parallel_channels * num_conv_layers\n",
        "    # for i in range(parallel_channels * num_conv_layers):\n",
        "    #   if (i + 1) % num_conv_layers == 0:\n",
        "    #     kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 2, 8)\n",
        "    #   else:\n",
        "    #     kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 3, 16)\n",
        "    # for i in range(num_conv_layers * parallel_channels):\n",
        "    #   output_channels[i] = trial.suggest_int(\"output_channels_\"+str(i), 4, 32, 4)\n",
        "    # drop = trial.suggest_float(\"drop\", 0.2, 0.5)  # Dropout probability\n",
        "    # linear_layers = trial.suggest_int(\"linear_layers\", 0, 3)  # Number of linear layers\n",
        "    # linear_depth = [int(trial.suggest_discrete_uniform(\"linear_depth_\"+str(i), 10, 400, 10))\n",
        "    #                 for i in range(linear_layers)]  # Depth of each linear layer\n",
        "\n",
        "\n",
        "    parallel_channels =  6\n",
        "    num_conv_layers =  2\n",
        "    #kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "    #output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "    kernel_size = [0] * parallel_channels * num_conv_layers\n",
        "    output_channels = [0] * parallel_channels * num_conv_layers\n",
        "    for i in range(parallel_channels * num_conv_layers):\n",
        "      if (i + 1) % num_conv_layers == 0:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 2, 8)\n",
        "      else:\n",
        "        kernel_size[i] = trial.suggest_int(\"kernel_size_\"+str(i), 3, 16)\n",
        "    for i in range(num_conv_layers * parallel_channels):\n",
        "      output_channels[i] = trial.suggest_int(\"output_channels_\"+str(i), 4, 32, 4)\n",
        "    drop = 0.2915637951496211\n",
        "    linear_layers = 0\n",
        "    linear_depth = 1\n",
        "    lr =  0.008900505464977875\n",
        "    num_classes = 2\n",
        "\n",
        "    # Generate the model\n",
        "    model = Net(trial, parallel_channels,\n",
        "                 kernel_size,\n",
        "                 num_conv_layers, output_channels,\n",
        "                 drop, linear_layers,\n",
        "                 linear_depth, num_classes, 16).to(device)\n",
        "\n",
        "    # Generate the optimizers\n",
        "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])  # Optimizers\n",
        "    #lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)                                 # Learning rates\n",
        "    #optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    lr =  0.008900505464977875\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "    num_epochs = 42\n",
        "\n",
        "    # Training of the model\n",
        "    max_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train(model, optimizer)  # Train the model\n",
        "        accuracy = test(model)   # Evaluate the model\n",
        "        if accuracy > max_acc:\n",
        "          max_acc = accuracy\n",
        "          torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        # For pruning (stops trial early if not promising)\n",
        "        trial.report(accuracy, epoch)\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Optimization study for a PyTorch CNN with Optuna\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Use cuda if available for faster computations\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Parameters ----------------------------------------------------------\n",
        "\n",
        "    batch_size_train = 24               # Batch size for training data\n",
        "    batch_size_test = 24               # Batch size for testing data\n",
        "    number_of_trials = 500                # Number of Optuna trials\n",
        "    limit_obs = False                      # Limit number of observations for faster computation\n",
        "\n",
        "    # *** Note: For more accurate results, do not limit the observations.\n",
        "    #           If not limited, however, it might take a very long time to run.\n",
        "    #           Another option is to limit the number of epochs. ***\n",
        "\n",
        "    if limit_obs:  # Limit number of observations\n",
        "        number_of_train_examples = 500 * batch_size_train  # Max train observations\n",
        "        number_of_test_examples = 5 * batch_size_test      # Max test observations\n",
        "    else:\n",
        "        number_of_train_examples = 60000                   # Max train observations\n",
        "        number_of_test_examples = 10000                    # Max test observations\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Make runs repeatable\n",
        "    random_seed = 1\n",
        "    torch.backends.cudnn.enabled = False  # Disable cuDNN use of nondeterministic algorithms\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Create an Optuna study to maximize test accuracy\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=number_of_trials)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Results\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Find number of pruned and completed trials\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    # Display the study statistics\n",
        "    print(\"\\nStudy statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    trial = study.best_trial\n",
        "    print(\"Best trial:\")\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    # Save results to csv file\n",
        "    df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Exclude columns\n",
        "    df = df.loc[df['state'] == 'COMPLETE']        # Keep only results that did not prune\n",
        "    df = df.drop('state', axis=1)                 # Exclude state column\n",
        "    df = df.sort_values('value')                  # Sort based on accuracy\n",
        "    df.to_csv('optuna_results.csv', index=False)  # Save to csv file\n",
        "\n",
        "    # Display results in a dataframe\n",
        "    print(\"\\nOverall Results (ordered by accuracy):\\n {}\".format(df))\n",
        "\n",
        "    # Find the most important hyperparameters\n",
        "    most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
        "\n",
        "    # Display the most important hyperparameters\n",
        "    print('\\nMost important hyperparameters:')\n",
        "    for key, value in most_important_parameters.items():\n",
        "        print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMT6Q2xjuGdD",
        "outputId": "e8faf6e3-2e76-47c6-c66a-8c44c5d8be07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-03 07:04:54,986] A new study created in memory with name: no-name-0641e689-f7f8-4f18-8648-f212790c50b3\n",
            "<ipython-input-17-b6b9e023db7f>:190: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  output_channels[i] = trial.suggest_int(\"output_channels_\"+str(i), 4, 32, 4)\n",
            "[I 2025-04-03 07:05:14,678] Trial 0 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 10, 'kernel_size_1': 3, 'kernel_size_2': 8, 'kernel_size_3': 5, 'kernel_size_4': 9, 'kernel_size_5': 6, 'kernel_size_6': 15, 'kernel_size_7': 6, 'kernel_size_8': 12, 'kernel_size_9': 7, 'kernel_size_10': 15, 'kernel_size_11': 5, 'output_channels_0': 4, 'output_channels_1': 20, 'output_channels_2': 8, 'output_channels_3': 28, 'output_channels_4': 20, 'output_channels_5': 12, 'output_channels_6': 12, 'output_channels_7': 16, 'output_channels_8': 24, 'output_channels_9': 12, 'output_channels_10': 20, 'output_channels_11': 28}. Best is trial 0 with value: 0.7759336099585062.\n",
            "[I 2025-04-03 07:05:34,913] Trial 1 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 6, 'kernel_size_2': 8, 'kernel_size_3': 8, 'kernel_size_4': 8, 'kernel_size_5': 4, 'kernel_size_6': 13, 'kernel_size_7': 5, 'kernel_size_8': 6, 'kernel_size_9': 8, 'kernel_size_10': 9, 'kernel_size_11': 4, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 28, 'output_channels_3': 24, 'output_channels_4': 8, 'output_channels_5': 8, 'output_channels_6': 12, 'output_channels_7': 12, 'output_channels_8': 12, 'output_channels_9': 32, 'output_channels_10': 4, 'output_channels_11': 28}. Best is trial 0 with value: 0.7759336099585062.\n",
            "[I 2025-04-03 07:05:56,997] Trial 2 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 4, 'kernel_size_2': 12, 'kernel_size_3': 7, 'kernel_size_4': 15, 'kernel_size_5': 7, 'kernel_size_6': 9, 'kernel_size_7': 7, 'kernel_size_8': 11, 'kernel_size_9': 6, 'kernel_size_10': 12, 'kernel_size_11': 5, 'output_channels_0': 16, 'output_channels_1': 20, 'output_channels_2': 12, 'output_channels_3': 20, 'output_channels_4': 20, 'output_channels_5': 12, 'output_channels_6': 8, 'output_channels_7': 20, 'output_channels_8': 20, 'output_channels_9': 28, 'output_channels_10': 20, 'output_channels_11': 4}. Best is trial 2 with value: 0.7842323651452282.\n",
            "[I 2025-04-03 07:06:16,364] Trial 3 finished with value: 0.7634854771784232 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 5, 'kernel_size_2': 3, 'kernel_size_3': 8, 'kernel_size_4': 5, 'kernel_size_5': 8, 'kernel_size_6': 9, 'kernel_size_7': 8, 'kernel_size_8': 4, 'kernel_size_9': 8, 'kernel_size_10': 10, 'kernel_size_11': 6, 'output_channels_0': 32, 'output_channels_1': 12, 'output_channels_2': 12, 'output_channels_3': 32, 'output_channels_4': 8, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 16, 'output_channels_8': 24, 'output_channels_9': 12, 'output_channels_10': 8, 'output_channels_11': 4}. Best is trial 2 with value: 0.7842323651452282.\n",
            "[I 2025-04-03 07:06:36,575] Trial 4 finished with value: 0.7800829875518672 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 3, 'kernel_size_2': 5, 'kernel_size_3': 2, 'kernel_size_4': 7, 'kernel_size_5': 5, 'kernel_size_6': 3, 'kernel_size_7': 8, 'kernel_size_8': 12, 'kernel_size_9': 3, 'kernel_size_10': 5, 'kernel_size_11': 5, 'output_channels_0': 16, 'output_channels_1': 32, 'output_channels_2': 24, 'output_channels_3': 8, 'output_channels_4': 32, 'output_channels_5': 16, 'output_channels_6': 28, 'output_channels_7': 12, 'output_channels_8': 32, 'output_channels_9': 20, 'output_channels_10': 4, 'output_channels_11': 24}. Best is trial 2 with value: 0.7842323651452282.\n",
            "[I 2025-04-03 07:06:37,482] Trial 5 pruned. \n",
            "[I 2025-04-03 07:06:38,383] Trial 6 pruned. \n",
            "[I 2025-04-03 07:06:38,839] Trial 7 pruned. \n",
            "[I 2025-04-03 07:06:39,744] Trial 8 pruned. \n",
            "[I 2025-04-03 07:06:40,654] Trial 9 pruned. \n",
            "[I 2025-04-03 07:06:41,190] Trial 10 pruned. \n",
            "[I 2025-04-03 07:06:42,218] Trial 11 pruned. \n",
            "[I 2025-04-03 07:06:42,760] Trial 12 pruned. \n",
            "[I 2025-04-03 07:07:03,026] Trial 13 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 4, 'kernel_size_2': 13, 'kernel_size_3': 7, 'kernel_size_4': 10, 'kernel_size_5': 5, 'kernel_size_6': 6, 'kernel_size_7': 8, 'kernel_size_8': 9, 'kernel_size_9': 6, 'kernel_size_10': 13, 'kernel_size_11': 3, 'output_channels_0': 8, 'output_channels_1': 28, 'output_channels_2': 12, 'output_channels_3': 8, 'output_channels_4': 24, 'output_channels_5': 4, 'output_channels_6': 24, 'output_channels_7': 12, 'output_channels_8': 16, 'output_channels_9': 24, 'output_channels_10': 16, 'output_channels_11': 12}. Best is trial 13 with value: 0.7883817427385892.\n",
            "[I 2025-04-03 07:07:03,559] Trial 14 pruned. \n",
            "[I 2025-04-03 07:07:04,112] Trial 15 pruned. \n",
            "[I 2025-04-03 07:07:23,682] Trial 16 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 3, 'kernel_size_1': 4, 'kernel_size_2': 15, 'kernel_size_3': 7, 'kernel_size_4': 10, 'kernel_size_5': 3, 'kernel_size_6': 5, 'kernel_size_7': 5, 'kernel_size_8': 10, 'kernel_size_9': 6, 'kernel_size_10': 13, 'kernel_size_11': 7, 'output_channels_0': 12, 'output_channels_1': 16, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 24, 'output_channels_5': 8, 'output_channels_6': 4, 'output_channels_7': 8, 'output_channels_8': 16, 'output_channels_9': 28, 'output_channels_10': 12, 'output_channels_11': 12}. Best is trial 13 with value: 0.7883817427385892.\n",
            "[I 2025-04-03 07:07:24,413] Trial 17 pruned. \n",
            "[I 2025-04-03 07:07:25,794] Trial 18 pruned. \n",
            "[I 2025-04-03 07:07:26,590] Trial 19 pruned. \n",
            "[I 2025-04-03 07:07:27,382] Trial 20 pruned. \n",
            "[I 2025-04-03 07:07:27,942] Trial 21 pruned. \n",
            "[I 2025-04-03 07:07:28,495] Trial 22 pruned. \n",
            "[I 2025-04-03 07:07:29,489] Trial 23 pruned. \n",
            "[I 2025-04-03 07:07:30,478] Trial 24 pruned. \n",
            "[I 2025-04-03 07:07:31,484] Trial 25 pruned. \n",
            "[I 2025-04-03 07:07:32,487] Trial 26 pruned. \n",
            "[I 2025-04-03 07:07:33,048] Trial 27 pruned. \n",
            "[I 2025-04-03 07:07:34,046] Trial 28 pruned. \n",
            "[I 2025-04-03 07:07:35,046] Trial 29 pruned. \n",
            "[I 2025-04-03 07:07:36,035] Trial 30 pruned. \n",
            "[I 2025-04-03 07:07:37,165] Trial 31 pruned. \n",
            "[I 2025-04-03 07:07:57,377] Trial 32 finished with value: 0.7925311203319502 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 3, 'kernel_size_2': 3, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 7, 'kernel_size_6': 15, 'kernel_size_7': 6, 'kernel_size_8': 11, 'kernel_size_9': 8, 'kernel_size_10': 15, 'kernel_size_11': 4, 'output_channels_0': 8, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 24, 'output_channels_4': 16, 'output_channels_5': 16, 'output_channels_6': 8, 'output_channels_7': 16, 'output_channels_8': 24, 'output_channels_9': 16, 'output_channels_10': 16, 'output_channels_11': 24}. Best is trial 32 with value: 0.7925311203319502.\n",
            "[I 2025-04-03 07:07:57,927] Trial 33 pruned. \n",
            "[I 2025-04-03 07:08:18,349] Trial 34 finished with value: 0.8257261410788381 and parameters: {'kernel_size_0': 7, 'kernel_size_1': 2, 'kernel_size_2': 4, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 8, 'kernel_size_6': 14, 'kernel_size_7': 6, 'kernel_size_8': 9, 'kernel_size_9': 8, 'kernel_size_10': 12, 'kernel_size_11': 3, 'output_channels_0': 12, 'output_channels_1': 28, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 8, 'output_channels_5': 20, 'output_channels_6': 4, 'output_channels_7': 20, 'output_channels_8': 28, 'output_channels_9': 20, 'output_channels_10': 12, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:08:19,002] Trial 35 pruned. \n",
            "[I 2025-04-03 07:08:38,628] Trial 36 finished with value: 0.7925311203319502 and parameters: {'kernel_size_0': 7, 'kernel_size_1': 2, 'kernel_size_2': 4, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 8, 'kernel_size_6': 15, 'kernel_size_7': 6, 'kernel_size_8': 9, 'kernel_size_9': 8, 'kernel_size_10': 13, 'kernel_size_11': 4, 'output_channels_0': 8, 'output_channels_1': 28, 'output_channels_2': 4, 'output_channels_3': 24, 'output_channels_4': 4, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 24, 'output_channels_8': 28, 'output_channels_9': 20, 'output_channels_10': 16, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:08:39,638] Trial 37 pruned. \n",
            "[I 2025-04-03 07:08:40,203] Trial 38 pruned. \n",
            "[I 2025-04-03 07:08:40,758] Trial 39 pruned. \n",
            "[I 2025-04-03 07:08:41,327] Trial 40 pruned. \n",
            "[I 2025-04-03 07:08:42,686] Trial 41 pruned. \n",
            "[I 2025-04-03 07:08:43,479] Trial 42 pruned. \n",
            "[I 2025-04-03 07:08:44,288] Trial 43 pruned. \n",
            "[I 2025-04-03 07:08:45,398] Trial 44 pruned. \n",
            "[I 2025-04-03 07:08:46,430] Trial 45 pruned. \n",
            "[I 2025-04-03 07:08:47,438] Trial 46 pruned. \n",
            "[I 2025-04-03 07:08:47,994] Trial 47 pruned. \n",
            "[I 2025-04-03 07:08:48,560] Trial 48 pruned. \n",
            "[I 2025-04-03 07:08:49,133] Trial 49 pruned. \n",
            "[I 2025-04-03 07:08:49,693] Trial 50 pruned. \n",
            "[I 2025-04-03 07:08:50,269] Trial 51 pruned. \n",
            "[I 2025-04-03 07:08:50,826] Trial 52 pruned. \n",
            "[I 2025-04-03 07:09:11,181] Trial 53 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 3, 'kernel_size_2': 3, 'kernel_size_3': 5, 'kernel_size_4': 15, 'kernel_size_5': 5, 'kernel_size_6': 11, 'kernel_size_7': 8, 'kernel_size_8': 11, 'kernel_size_9': 4, 'kernel_size_10': 13, 'kernel_size_11': 6, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 16, 'output_channels_3': 4, 'output_channels_4': 32, 'output_channels_5': 16, 'output_channels_6': 12, 'output_channels_7': 8, 'output_channels_8': 28, 'output_channels_9': 20, 'output_channels_10': 4, 'output_channels_11': 32}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:09:30,656] Trial 54 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 5, 'kernel_size_2': 5, 'kernel_size_3': 7, 'kernel_size_4': 9, 'kernel_size_5': 6, 'kernel_size_6': 16, 'kernel_size_7': 8, 'kernel_size_8': 15, 'kernel_size_9': 7, 'kernel_size_10': 9, 'kernel_size_11': 6, 'output_channels_0': 12, 'output_channels_1': 28, 'output_channels_2': 4, 'output_channels_3': 4, 'output_channels_4': 8, 'output_channels_5': 20, 'output_channels_6': 32, 'output_channels_7': 16, 'output_channels_8': 28, 'output_channels_9': 24, 'output_channels_10': 8, 'output_channels_11': 16}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:09:51,065] Trial 55 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 13, 'kernel_size_1': 2, 'kernel_size_2': 4, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 5, 'kernel_size_6': 5, 'kernel_size_7': 7, 'kernel_size_8': 13, 'kernel_size_9': 3, 'kernel_size_10': 14, 'kernel_size_11': 5, 'output_channels_0': 16, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 20, 'output_channels_5': 16, 'output_channels_6': 20, 'output_channels_7': 16, 'output_channels_8': 32, 'output_channels_9': 20, 'output_channels_10': 12, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:09:51,635] Trial 56 pruned. \n",
            "[I 2025-04-03 07:09:52,227] Trial 57 pruned. \n",
            "[I 2025-04-03 07:09:53,249] Trial 58 pruned. \n",
            "[I 2025-04-03 07:09:53,815] Trial 59 pruned. \n",
            "[I 2025-04-03 07:09:54,394] Trial 60 pruned. \n",
            "[I 2025-04-03 07:09:54,957] Trial 61 pruned. \n",
            "[I 2025-04-03 07:09:55,553] Trial 62 pruned. \n",
            "[I 2025-04-03 07:09:56,124] Trial 63 pruned. \n",
            "[I 2025-04-03 07:09:57,149] Trial 64 pruned. \n",
            "[I 2025-04-03 07:09:57,718] Trial 65 pruned. \n",
            "[I 2025-04-03 07:09:58,742] Trial 66 pruned. \n",
            "[I 2025-04-03 07:10:00,001] Trial 67 pruned. \n",
            "[I 2025-04-03 07:10:00,831] Trial 68 pruned. \n",
            "[I 2025-04-03 07:10:21,817] Trial 69 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 4, 'kernel_size_2': 8, 'kernel_size_3': 7, 'kernel_size_4': 8, 'kernel_size_5': 6, 'kernel_size_6': 12, 'kernel_size_7': 2, 'kernel_size_8': 10, 'kernel_size_9': 6, 'kernel_size_10': 7, 'kernel_size_11': 6, 'output_channels_0': 28, 'output_channels_1': 24, 'output_channels_2': 16, 'output_channels_3': 20, 'output_channels_4': 12, 'output_channels_5': 12, 'output_channels_6': 8, 'output_channels_7': 24, 'output_channels_8': 16, 'output_channels_9': 24, 'output_channels_10': 16, 'output_channels_11': 24}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:10:22,400] Trial 70 pruned. \n",
            "[I 2025-04-03 07:10:37,107] Trial 71 pruned. \n",
            "[I 2025-04-03 07:10:37,687] Trial 72 pruned. \n",
            "[I 2025-04-03 07:10:38,372] Trial 73 pruned. \n",
            "[I 2025-04-03 07:10:39,800] Trial 74 pruned. \n",
            "[I 2025-04-03 07:10:41,189] Trial 75 pruned. \n",
            "[I 2025-04-03 07:10:41,764] Trial 76 pruned. \n",
            "[I 2025-04-03 07:10:42,332] Trial 77 pruned. \n",
            "[I 2025-04-03 07:10:42,958] Trial 78 pruned. \n",
            "[I 2025-04-03 07:10:43,529] Trial 79 pruned. \n",
            "[I 2025-04-03 07:11:03,091] Trial 80 finished with value: 0.7676348547717843 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 7, 'kernel_size_2': 7, 'kernel_size_3': 5, 'kernel_size_4': 8, 'kernel_size_5': 7, 'kernel_size_6': 14, 'kernel_size_7': 5, 'kernel_size_8': 10, 'kernel_size_9': 8, 'kernel_size_10': 4, 'kernel_size_11': 5, 'output_channels_0': 12, 'output_channels_1': 28, 'output_channels_2': 12, 'output_channels_3': 16, 'output_channels_4': 12, 'output_channels_5': 32, 'output_channels_6': 8, 'output_channels_7': 24, 'output_channels_8': 32, 'output_channels_9': 16, 'output_channels_10': 16, 'output_channels_11': 8}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:11:03,685] Trial 81 pruned. \n",
            "[I 2025-04-03 07:11:04,413] Trial 82 pruned. \n",
            "[I 2025-04-03 07:11:05,267] Trial 83 pruned. \n",
            "[I 2025-04-03 07:11:06,077] Trial 84 pruned. \n",
            "[I 2025-04-03 07:11:06,946] Trial 85 pruned. \n",
            "[I 2025-04-03 07:11:07,975] Trial 86 pruned. \n",
            "[I 2025-04-03 07:11:09,009] Trial 87 pruned. \n",
            "[I 2025-04-03 07:11:09,600] Trial 88 pruned. \n",
            "[I 2025-04-03 07:11:10,173] Trial 89 pruned. \n",
            "[I 2025-04-03 07:11:11,214] Trial 90 pruned. \n",
            "[I 2025-04-03 07:11:11,793] Trial 91 pruned. \n",
            "[I 2025-04-03 07:11:12,368] Trial 92 pruned. \n",
            "[I 2025-04-03 07:11:13,428] Trial 93 pruned. \n",
            "[I 2025-04-03 07:11:14,447] Trial 94 pruned. \n",
            "[I 2025-04-03 07:11:15,044] Trial 95 pruned. \n",
            "[I 2025-04-03 07:11:16,075] Trial 96 pruned. \n",
            "[I 2025-04-03 07:11:16,663] Trial 97 pruned. \n",
            "[I 2025-04-03 07:11:17,385] Trial 98 pruned. \n",
            "[I 2025-04-03 07:11:18,829] Trial 99 pruned. \n",
            "[I 2025-04-03 07:11:20,206] Trial 100 pruned. \n",
            "[I 2025-04-03 07:11:20,805] Trial 101 pruned. \n",
            "[I 2025-04-03 07:11:21,393] Trial 102 pruned. \n",
            "[I 2025-04-03 07:11:21,991] Trial 103 pruned. \n",
            "[I 2025-04-03 07:11:23,026] Trial 104 pruned. \n",
            "[I 2025-04-03 07:11:23,607] Trial 105 pruned. \n",
            "[I 2025-04-03 07:11:24,206] Trial 106 pruned. \n",
            "[I 2025-04-03 07:11:25,236] Trial 107 pruned. \n",
            "[I 2025-04-03 07:11:25,825] Trial 108 pruned. \n",
            "[I 2025-04-03 07:11:26,422] Trial 109 pruned. \n",
            "[I 2025-04-03 07:11:27,006] Trial 110 pruned. \n",
            "[I 2025-04-03 07:11:27,588] Trial 111 pruned. \n",
            "[I 2025-04-03 07:11:28,179] Trial 112 pruned. \n",
            "[I 2025-04-03 07:11:28,761] Trial 113 pruned. \n",
            "[I 2025-04-03 07:11:29,360] Trial 114 pruned. \n",
            "[I 2025-04-03 07:11:30,577] Trial 115 pruned. \n",
            "[I 2025-04-03 07:11:31,480] Trial 116 pruned. \n",
            "[I 2025-04-03 07:11:32,330] Trial 117 pruned. \n",
            "[I 2025-04-03 07:11:33,160] Trial 118 pruned. \n",
            "[I 2025-04-03 07:11:33,753] Trial 119 pruned. \n",
            "[I 2025-04-03 07:11:34,347] Trial 120 pruned. \n",
            "[I 2025-04-03 07:11:35,382] Trial 121 pruned. \n",
            "[I 2025-04-03 07:11:35,963] Trial 122 pruned. \n",
            "[I 2025-04-03 07:11:55,607] Trial 123 finished with value: 0.7676348547717843 and parameters: {'kernel_size_0': 15, 'kernel_size_1': 2, 'kernel_size_2': 5, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 5, 'kernel_size_6': 7, 'kernel_size_7': 6, 'kernel_size_8': 14, 'kernel_size_9': 3, 'kernel_size_10': 13, 'kernel_size_11': 7, 'output_channels_0': 16, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 24, 'output_channels_5': 16, 'output_channels_6': 24, 'output_channels_7': 16, 'output_channels_8': 32, 'output_channels_9': 20, 'output_channels_10': 16, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:11:56,329] Trial 124 pruned. \n",
            "[I 2025-04-03 07:12:16,688] Trial 125 finished with value: 0.7593360995850622 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 2, 'kernel_size_2': 11, 'kernel_size_3': 8, 'kernel_size_4': 6, 'kernel_size_5': 2, 'kernel_size_6': 14, 'kernel_size_7': 8, 'kernel_size_8': 13, 'kernel_size_9': 3, 'kernel_size_10': 15, 'kernel_size_11': 3, 'output_channels_0': 20, 'output_channels_1': 28, 'output_channels_2': 12, 'output_channels_3': 24, 'output_channels_4': 16, 'output_channels_5': 20, 'output_channels_6': 20, 'output_channels_7': 12, 'output_channels_8': 12, 'output_channels_9': 20, 'output_channels_10': 16, 'output_channels_11': 32}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:12:17,279] Trial 126 pruned. \n",
            "[I 2025-04-03 07:12:18,338] Trial 127 pruned. \n",
            "[I 2025-04-03 07:12:18,931] Trial 128 pruned. \n",
            "[I 2025-04-03 07:12:39,597] Trial 129 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 2, 'kernel_size_2': 8, 'kernel_size_3': 2, 'kernel_size_4': 7, 'kernel_size_5': 7, 'kernel_size_6': 15, 'kernel_size_7': 4, 'kernel_size_8': 8, 'kernel_size_9': 6, 'kernel_size_10': 14, 'kernel_size_11': 6, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 8, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 8, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 24, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:12:59,294] Trial 130 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 4, 'kernel_size_2': 8, 'kernel_size_3': 2, 'kernel_size_4': 9, 'kernel_size_5': 7, 'kernel_size_6': 15, 'kernel_size_7': 5, 'kernel_size_8': 8, 'kernel_size_9': 6, 'kernel_size_10': 10, 'kernel_size_11': 6, 'output_channels_0': 20, 'output_channels_1': 24, 'output_channels_2': 4, 'output_channels_3': 24, 'output_channels_4': 8, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 8, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 24, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:12:59,900] Trial 131 pruned. \n",
            "[I 2025-04-03 07:13:01,092] Trial 132 pruned. \n",
            "[I 2025-04-03 07:13:01,975] Trial 133 pruned. \n",
            "[I 2025-04-03 07:13:02,822] Trial 134 pruned. \n",
            "[I 2025-04-03 07:13:04,175] Trial 135 pruned. \n",
            "[I 2025-04-03 07:13:04,767] Trial 136 pruned. \n",
            "[I 2025-04-03 07:13:05,814] Trial 137 pruned. \n",
            "[I 2025-04-03 07:13:06,425] Trial 138 pruned. \n",
            "[I 2025-04-03 07:13:07,475] Trial 139 pruned. \n",
            "[I 2025-04-03 07:13:08,087] Trial 140 pruned. \n",
            "[I 2025-04-03 07:13:08,694] Trial 141 pruned. \n",
            "[I 2025-04-03 07:13:28,928] Trial 142 finished with value: 0.7717842323651453 and parameters: {'kernel_size_0': 9, 'kernel_size_1': 2, 'kernel_size_2': 4, 'kernel_size_3': 7, 'kernel_size_4': 8, 'kernel_size_5': 6, 'kernel_size_6': 15, 'kernel_size_7': 6, 'kernel_size_8': 11, 'kernel_size_9': 6, 'kernel_size_10': 14, 'kernel_size_11': 4, 'output_channels_0': 4, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 32, 'output_channels_5': 16, 'output_channels_6': 8, 'output_channels_7': 12, 'output_channels_8': 24, 'output_channels_9': 20, 'output_channels_10': 16, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:13:29,729] Trial 143 pruned. \n",
            "[I 2025-04-03 07:13:30,320] Trial 144 pruned. \n",
            "[I 2025-04-03 07:13:30,933] Trial 145 pruned. \n",
            "[I 2025-04-03 07:13:31,535] Trial 146 pruned. \n",
            "[I 2025-04-03 07:13:32,153] Trial 147 pruned. \n",
            "[I 2025-04-03 07:13:32,768] Trial 148 pruned. \n",
            "[I 2025-04-03 07:13:33,359] Trial 149 pruned. \n",
            "[I 2025-04-03 07:13:33,968] Trial 150 pruned. \n",
            "[I 2025-04-03 07:13:34,566] Trial 151 pruned. \n",
            "[I 2025-04-03 07:13:35,621] Trial 152 pruned. \n",
            "[I 2025-04-03 07:13:36,245] Trial 153 pruned. \n",
            "[I 2025-04-03 07:13:36,856] Trial 154 pruned. \n",
            "[I 2025-04-03 07:13:37,452] Trial 155 pruned. \n",
            "[I 2025-04-03 07:13:38,514] Trial 156 pruned. \n",
            "[I 2025-04-03 07:13:39,648] Trial 157 pruned. \n",
            "[I 2025-04-03 07:13:40,524] Trial 158 pruned. \n",
            "[I 2025-04-03 07:13:41,399] Trial 159 pruned. \n",
            "[I 2025-04-03 07:13:42,325] Trial 160 pruned. \n",
            "[I 2025-04-03 07:13:43,016] Trial 161 pruned. \n",
            "[I 2025-04-03 07:13:43,613] Trial 162 pruned. \n",
            "[I 2025-04-03 07:13:44,264] Trial 163 pruned. \n",
            "[I 2025-04-03 07:13:44,873] Trial 164 pruned. \n",
            "[I 2025-04-03 07:13:45,476] Trial 165 pruned. \n",
            "[I 2025-04-03 07:13:46,097] Trial 166 pruned. \n",
            "[I 2025-04-03 07:14:05,770] Trial 167 finished with value: 0.7717842323651453 and parameters: {'kernel_size_0': 12, 'kernel_size_1': 2, 'kernel_size_2': 7, 'kernel_size_3': 7, 'kernel_size_4': 7, 'kernel_size_5': 8, 'kernel_size_6': 15, 'kernel_size_7': 4, 'kernel_size_8': 10, 'kernel_size_9': 6, 'kernel_size_10': 12, 'kernel_size_11': 2, 'output_channels_0': 20, 'output_channels_1': 24, 'output_channels_2': 12, 'output_channels_3': 20, 'output_channels_4': 28, 'output_channels_5': 12, 'output_channels_6': 16, 'output_channels_7': 8, 'output_channels_8': 32, 'output_channels_9': 28, 'output_channels_10': 16, 'output_channels_11': 12}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:14:06,647] Trial 168 pruned. \n",
            "[I 2025-04-03 07:14:07,502] Trial 169 pruned. \n",
            "[I 2025-04-03 07:14:08,433] Trial 170 pruned. \n",
            "[I 2025-04-03 07:14:09,050] Trial 171 pruned. \n",
            "[I 2025-04-03 07:14:09,664] Trial 172 pruned. \n",
            "[I 2025-04-03 07:14:10,276] Trial 173 pruned. \n",
            "[I 2025-04-03 07:14:11,335] Trial 174 pruned. \n",
            "[I 2025-04-03 07:14:12,394] Trial 175 pruned. \n",
            "[I 2025-04-03 07:14:13,014] Trial 176 pruned. \n",
            "[I 2025-04-03 07:14:13,623] Trial 177 pruned. \n",
            "[I 2025-04-03 07:14:14,282] Trial 178 pruned. \n",
            "[I 2025-04-03 07:14:14,918] Trial 179 pruned. \n",
            "[I 2025-04-03 07:14:15,520] Trial 180 pruned. \n",
            "[I 2025-04-03 07:14:16,140] Trial 181 pruned. \n",
            "[I 2025-04-03 07:14:16,742] Trial 182 pruned. \n",
            "[I 2025-04-03 07:14:17,830] Trial 183 pruned. \n",
            "[I 2025-04-03 07:14:18,499] Trial 184 pruned. \n",
            "[I 2025-04-03 07:14:19,393] Trial 185 pruned. \n",
            "[I 2025-04-03 07:14:20,284] Trial 186 pruned. \n",
            "[I 2025-04-03 07:14:40,202] Trial 187 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 2, 'kernel_size_2': 3, 'kernel_size_3': 4, 'kernel_size_4': 7, 'kernel_size_5': 2, 'kernel_size_6': 14, 'kernel_size_7': 7, 'kernel_size_8': 9, 'kernel_size_9': 3, 'kernel_size_10': 13, 'kernel_size_11': 3, 'output_channels_0': 16, 'output_channels_1': 32, 'output_channels_2': 16, 'output_channels_3': 16, 'output_channels_4': 32, 'output_channels_5': 12, 'output_channels_6': 24, 'output_channels_7': 8, 'output_channels_8': 32, 'output_channels_9': 28, 'output_channels_10': 4, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:14:40,818] Trial 188 pruned. \n",
            "[I 2025-04-03 07:14:41,423] Trial 189 pruned. \n",
            "[I 2025-04-03 07:14:42,044] Trial 190 pruned. \n",
            "[I 2025-04-03 07:14:42,664] Trial 191 pruned. \n",
            "[I 2025-04-03 07:14:43,291] Trial 192 pruned. \n",
            "[I 2025-04-03 07:14:43,915] Trial 193 pruned. \n",
            "[I 2025-04-03 07:14:44,712] Trial 194 pruned. \n",
            "[I 2025-04-03 07:14:46,312] Trial 195 pruned. \n",
            "[I 2025-04-03 07:14:47,928] Trial 196 pruned. \n",
            "[I 2025-04-03 07:15:08,284] Trial 197 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 4, 'kernel_size_1': 2, 'kernel_size_2': 7, 'kernel_size_3': 7, 'kernel_size_4': 6, 'kernel_size_5': 5, 'kernel_size_6': 8, 'kernel_size_7': 8, 'kernel_size_8': 11, 'kernel_size_9': 3, 'kernel_size_10': 9, 'kernel_size_11': 4, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 24, 'output_channels_5': 28, 'output_channels_6': 8, 'output_channels_7': 12, 'output_channels_8': 24, 'output_channels_9': 20, 'output_channels_10': 4, 'output_channels_11': 16}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:15:08,905] Trial 198 pruned. \n",
            "[I 2025-04-03 07:15:09,524] Trial 199 pruned. \n",
            "[I 2025-04-03 07:15:10,152] Trial 200 pruned. \n",
            "[I 2025-04-03 07:15:11,216] Trial 201 pruned. \n",
            "[I 2025-04-03 07:15:11,983] Trial 202 pruned. \n",
            "[I 2025-04-03 07:15:12,892] Trial 203 pruned. \n",
            "[I 2025-04-03 07:15:13,793] Trial 204 pruned. \n",
            "[I 2025-04-03 07:15:14,692] Trial 205 pruned. \n",
            "[I 2025-04-03 07:15:15,328] Trial 206 pruned. \n",
            "[I 2025-04-03 07:15:16,394] Trial 207 pruned. \n",
            "[I 2025-04-03 07:15:17,012] Trial 208 pruned. \n",
            "[I 2025-04-03 07:15:17,624] Trial 209 pruned. \n",
            "[I 2025-04-03 07:15:18,278] Trial 210 pruned. \n",
            "[I 2025-04-03 07:15:18,902] Trial 211 pruned. \n",
            "[I 2025-04-03 07:15:19,976] Trial 212 pruned. \n",
            "[I 2025-04-03 07:15:20,601] Trial 213 pruned. \n",
            "[I 2025-04-03 07:15:21,252] Trial 214 pruned. \n",
            "[I 2025-04-03 07:15:21,867] Trial 215 pruned. \n",
            "[I 2025-04-03 07:15:42,223] Trial 216 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 7, 'kernel_size_1': 4, 'kernel_size_2': 12, 'kernel_size_3': 8, 'kernel_size_4': 4, 'kernel_size_5': 8, 'kernel_size_6': 15, 'kernel_size_7': 8, 'kernel_size_8': 3, 'kernel_size_9': 6, 'kernel_size_10': 7, 'kernel_size_11': 6, 'output_channels_0': 16, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 4, 'output_channels_4': 12, 'output_channels_5': 12, 'output_channels_6': 12, 'output_channels_7': 12, 'output_channels_8': 20, 'output_channels_9': 20, 'output_channels_10': 4, 'output_channels_11': 20}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:15:43,298] Trial 217 pruned. \n",
            "[I 2025-04-03 07:15:43,941] Trial 218 pruned. \n",
            "[I 2025-04-03 07:15:44,589] Trial 219 pruned. \n",
            "[I 2025-04-03 07:15:45,220] Trial 220 pruned. \n",
            "[I 2025-04-03 07:15:45,849] Trial 221 pruned. \n",
            "[I 2025-04-03 07:15:46,476] Trial 222 pruned. \n",
            "[I 2025-04-03 07:15:47,127] Trial 223 pruned. \n",
            "[I 2025-04-03 07:15:47,765] Trial 224 pruned. \n",
            "[I 2025-04-03 07:15:48,392] Trial 225 pruned. \n",
            "[I 2025-04-03 07:15:49,034] Trial 226 pruned. \n",
            "[I 2025-04-03 07:15:49,666] Trial 227 pruned. \n",
            "[I 2025-04-03 07:15:50,298] Trial 228 pruned. \n",
            "[I 2025-04-03 07:15:51,140] Trial 229 pruned. \n",
            "[I 2025-04-03 07:15:52,071] Trial 230 pruned. \n",
            "[I 2025-04-03 07:15:53,579] Trial 231 pruned. \n",
            "[I 2025-04-03 07:15:54,220] Trial 232 pruned. \n",
            "[I 2025-04-03 07:15:54,846] Trial 233 pruned. \n",
            "[I 2025-04-03 07:16:14,416] Trial 234 finished with value: 0.8008298755186722 and parameters: {'kernel_size_0': 8, 'kernel_size_1': 2, 'kernel_size_2': 12, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 2, 'kernel_size_6': 13, 'kernel_size_7': 8, 'kernel_size_8': 11, 'kernel_size_9': 3, 'kernel_size_10': 16, 'kernel_size_11': 3, 'output_channels_0': 20, 'output_channels_1': 28, 'output_channels_2': 12, 'output_channels_3': 24, 'output_channels_4': 12, 'output_channels_5': 20, 'output_channels_6': 8, 'output_channels_7': 12, 'output_channels_8': 12, 'output_channels_9': 20, 'output_channels_10': 16, 'output_channels_11': 32}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:16:15,535] Trial 235 pruned. \n",
            "[I 2025-04-03 07:16:16,161] Trial 236 pruned. \n",
            "[I 2025-04-03 07:16:16,988] Trial 237 pruned. \n",
            "[I 2025-04-03 07:16:17,928] Trial 238 pruned. \n",
            "[I 2025-04-03 07:16:18,853] Trial 239 pruned. \n",
            "[I 2025-04-03 07:16:19,677] Trial 240 pruned. \n",
            "[I 2025-04-03 07:16:20,306] Trial 241 pruned. \n",
            "[I 2025-04-03 07:16:21,405] Trial 242 pruned. \n",
            "[I 2025-04-03 07:16:22,044] Trial 243 pruned. \n",
            "[I 2025-04-03 07:16:22,668] Trial 244 pruned. \n",
            "[I 2025-04-03 07:16:23,318] Trial 245 pruned. \n",
            "[I 2025-04-03 07:16:23,970] Trial 246 pruned. \n",
            "[I 2025-04-03 07:16:44,128] Trial 247 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 6, 'kernel_size_2': 3, 'kernel_size_3': 8, 'kernel_size_4': 8, 'kernel_size_5': 2, 'kernel_size_6': 11, 'kernel_size_7': 5, 'kernel_size_8': 10, 'kernel_size_9': 3, 'kernel_size_10': 7, 'kernel_size_11': 3, 'output_channels_0': 24, 'output_channels_1': 20, 'output_channels_2': 12, 'output_channels_3': 20, 'output_channels_4': 32, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 16, 'output_channels_8': 20, 'output_channels_9': 16, 'output_channels_10': 16, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:16:45,614] Trial 248 pruned. \n",
            "[I 2025-04-03 07:16:46,243] Trial 249 pruned. \n",
            "[I 2025-04-03 07:17:05,826] Trial 250 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 5, 'kernel_size_1': 6, 'kernel_size_2': 3, 'kernel_size_3': 8, 'kernel_size_4': 9, 'kernel_size_5': 8, 'kernel_size_6': 10, 'kernel_size_7': 5, 'kernel_size_8': 10, 'kernel_size_9': 8, 'kernel_size_10': 7, 'kernel_size_11': 5, 'output_channels_0': 24, 'output_channels_1': 20, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 32, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 16, 'output_channels_8': 20, 'output_channels_9': 16, 'output_channels_10': 8, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:17:06,461] Trial 251 pruned. \n",
            "[I 2025-04-03 07:17:07,102] Trial 252 pruned. \n",
            "[I 2025-04-03 07:17:07,732] Trial 253 pruned. \n",
            "[I 2025-04-03 07:17:08,468] Trial 254 pruned. \n",
            "[I 2025-04-03 07:17:10,020] Trial 255 pruned. \n",
            "[I 2025-04-03 07:17:10,973] Trial 256 pruned. \n",
            "[I 2025-04-03 07:17:11,697] Trial 257 pruned. \n",
            "[I 2025-04-03 07:17:12,332] Trial 258 pruned. \n",
            "[I 2025-04-03 07:17:12,955] Trial 259 pruned. \n",
            "[I 2025-04-03 07:17:32,693] Trial 260 finished with value: 0.7966804979253111 and parameters: {'kernel_size_0': 12, 'kernel_size_1': 4, 'kernel_size_2': 3, 'kernel_size_3': 7, 'kernel_size_4': 7, 'kernel_size_5': 8, 'kernel_size_6': 6, 'kernel_size_7': 5, 'kernel_size_8': 9, 'kernel_size_9': 6, 'kernel_size_10': 12, 'kernel_size_11': 4, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 24, 'output_channels_5': 12, 'output_channels_6': 12, 'output_channels_7': 4, 'output_channels_8': 20, 'output_channels_9': 28, 'output_channels_10': 16, 'output_channels_11': 12}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:17:33,369] Trial 261 pruned. \n",
            "[I 2025-04-03 07:17:34,612] Trial 262 pruned. \n",
            "[I 2025-04-03 07:17:35,565] Trial 263 pruned. \n",
            "[I 2025-04-03 07:17:36,489] Trial 264 pruned. \n",
            "[I 2025-04-03 07:17:37,376] Trial 265 pruned. \n",
            "[I 2025-04-03 07:17:57,119] Trial 266 finished with value: 0.7966804979253111 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 4, 'kernel_size_2': 4, 'kernel_size_3': 7, 'kernel_size_4': 8, 'kernel_size_5': 8, 'kernel_size_6': 15, 'kernel_size_7': 5, 'kernel_size_8': 11, 'kernel_size_9': 6, 'kernel_size_10': 7, 'kernel_size_11': 4, 'output_channels_0': 8, 'output_channels_1': 20, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 32, 'output_channels_5': 24, 'output_channels_6': 8, 'output_channels_7': 8, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 4, 'output_channels_11': 8}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:17:57,765] Trial 267 pruned. \n",
            "[I 2025-04-03 07:17:58,401] Trial 268 pruned. \n",
            "[I 2025-04-03 07:17:59,079] Trial 269 pruned. \n",
            "[I 2025-04-03 07:17:59,725] Trial 270 pruned. \n",
            "[I 2025-04-03 07:18:00,521] Trial 271 pruned. \n",
            "[I 2025-04-03 07:18:01,474] Trial 272 pruned. \n",
            "[I 2025-04-03 07:18:02,409] Trial 273 pruned. \n",
            "[I 2025-04-03 07:18:03,299] Trial 274 pruned. \n",
            "[I 2025-04-03 07:18:03,957] Trial 275 pruned. \n",
            "[I 2025-04-03 07:18:04,606] Trial 276 pruned. \n",
            "[I 2025-04-03 07:18:05,262] Trial 277 pruned. \n",
            "[I 2025-04-03 07:18:05,913] Trial 278 pruned. \n",
            "[I 2025-04-03 07:18:06,561] Trial 279 pruned. \n",
            "[I 2025-04-03 07:18:07,222] Trial 280 pruned. \n",
            "[I 2025-04-03 07:18:07,874] Trial 281 pruned. \n",
            "[I 2025-04-03 07:18:08,523] Trial 282 pruned. \n",
            "[I 2025-04-03 07:18:09,184] Trial 283 pruned. \n",
            "[I 2025-04-03 07:18:20,408] Trial 284 pruned. \n",
            "[I 2025-04-03 07:18:21,062] Trial 285 pruned. \n",
            "[I 2025-04-03 07:18:41,564] Trial 286 finished with value: 0.7717842323651453 and parameters: {'kernel_size_0': 6, 'kernel_size_1': 2, 'kernel_size_2': 4, 'kernel_size_3': 2, 'kernel_size_4': 10, 'kernel_size_5': 5, 'kernel_size_6': 11, 'kernel_size_7': 8, 'kernel_size_8': 11, 'kernel_size_9': 3, 'kernel_size_10': 13, 'kernel_size_11': 4, 'output_channels_0': 16, 'output_channels_1': 24, 'output_channels_2': 32, 'output_channels_3': 24, 'output_channels_4': 4, 'output_channels_5': 20, 'output_channels_6': 32, 'output_channels_7': 16, 'output_channels_8': 20, 'output_channels_9': 28, 'output_channels_10': 12, 'output_channels_11': 12}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:18:42,367] Trial 287 pruned. \n",
            "[I 2025-04-03 07:18:43,019] Trial 288 pruned. \n",
            "[I 2025-04-03 07:18:43,667] Trial 289 pruned. \n",
            "[I 2025-04-03 07:19:03,431] Trial 290 finished with value: 0.7842323651452282 and parameters: {'kernel_size_0': 10, 'kernel_size_1': 4, 'kernel_size_2': 4, 'kernel_size_3': 8, 'kernel_size_4': 7, 'kernel_size_5': 4, 'kernel_size_6': 4, 'kernel_size_7': 8, 'kernel_size_8': 10, 'kernel_size_9': 6, 'kernel_size_10': 12, 'kernel_size_11': 5, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 4, 'output_channels_3': 20, 'output_channels_4': 20, 'output_channels_5': 8, 'output_channels_6': 8, 'output_channels_7': 12, 'output_channels_8': 28, 'output_channels_9': 24, 'output_channels_10': 16, 'output_channels_11': 28}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:19:04,097] Trial 291 pruned. \n",
            "[I 2025-04-03 07:19:04,752] Trial 292 pruned. \n",
            "[I 2025-04-03 07:19:05,646] Trial 293 pruned. \n",
            "[I 2025-04-03 07:19:06,595] Trial 294 pruned. \n",
            "[I 2025-04-03 07:19:07,556] Trial 295 pruned. \n",
            "[I 2025-04-03 07:19:08,306] Trial 296 pruned. \n",
            "[I 2025-04-03 07:19:08,985] Trial 297 pruned. \n",
            "[I 2025-04-03 07:19:10,094] Trial 298 pruned. \n",
            "[I 2025-04-03 07:19:10,752] Trial 299 pruned. \n",
            "[I 2025-04-03 07:19:11,407] Trial 300 pruned. \n",
            "[I 2025-04-03 07:19:12,073] Trial 301 pruned. \n",
            "[I 2025-04-03 07:19:12,732] Trial 302 pruned. \n",
            "[I 2025-04-03 07:19:13,406] Trial 303 pruned. \n",
            "[I 2025-04-03 07:19:14,073] Trial 304 pruned. \n",
            "[I 2025-04-03 07:19:14,729] Trial 305 pruned. \n",
            "[I 2025-04-03 07:19:15,401] Trial 306 pruned. \n",
            "[I 2025-04-03 07:19:16,112] Trial 307 pruned. \n",
            "[I 2025-04-03 07:19:16,766] Trial 308 pruned. \n",
            "[I 2025-04-03 07:19:17,451] Trial 309 pruned. \n",
            "[I 2025-04-03 07:19:18,347] Trial 310 pruned. \n",
            "[I 2025-04-03 07:19:19,449] Trial 311 pruned. \n",
            "[I 2025-04-03 07:19:20,642] Trial 312 pruned. \n",
            "[I 2025-04-03 07:19:21,805] Trial 313 pruned. \n",
            "[I 2025-04-03 07:19:22,951] Trial 314 pruned. \n",
            "[I 2025-04-03 07:19:42,635] Trial 315 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 10, 'kernel_size_1': 4, 'kernel_size_2': 4, 'kernel_size_3': 2, 'kernel_size_4': 7, 'kernel_size_5': 3, 'kernel_size_6': 3, 'kernel_size_7': 5, 'kernel_size_8': 9, 'kernel_size_9': 6, 'kernel_size_10': 12, 'kernel_size_11': 6, 'output_channels_0': 20, 'output_channels_1': 32, 'output_channels_2': 8, 'output_channels_3': 20, 'output_channels_4': 12, 'output_channels_5': 16, 'output_channels_6': 12, 'output_channels_7': 8, 'output_channels_8': 24, 'output_channels_9': 8, 'output_channels_10': 4, 'output_channels_11': 32}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:19:43,306] Trial 316 pruned. \n",
            "[I 2025-04-03 07:19:43,960] Trial 317 pruned. \n",
            "[I 2025-04-03 07:19:44,612] Trial 318 pruned. \n",
            "[I 2025-04-03 07:19:45,295] Trial 319 pruned. \n",
            "[I 2025-04-03 07:19:46,137] Trial 320 pruned. \n",
            "[I 2025-04-03 07:19:47,144] Trial 321 pruned. \n",
            "[I 2025-04-03 07:19:48,118] Trial 322 pruned. \n",
            "[I 2025-04-03 07:19:48,995] Trial 323 pruned. \n",
            "[I 2025-04-03 07:19:49,669] Trial 324 pruned. \n",
            "[I 2025-04-03 07:19:50,776] Trial 325 pruned. \n",
            "[I 2025-04-03 07:19:51,449] Trial 326 pruned. \n",
            "[I 2025-04-03 07:20:11,108] Trial 327 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 4, 'kernel_size_2': 11, 'kernel_size_3': 8, 'kernel_size_4': 12, 'kernel_size_5': 7, 'kernel_size_6': 13, 'kernel_size_7': 8, 'kernel_size_8': 9, 'kernel_size_9': 8, 'kernel_size_10': 13, 'kernel_size_11': 6, 'output_channels_0': 8, 'output_channels_1': 32, 'output_channels_2': 20, 'output_channels_3': 20, 'output_channels_4': 20, 'output_channels_5': 16, 'output_channels_6': 28, 'output_channels_7': 16, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 4, 'output_channels_11': 16}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:20:11,858] Trial 328 pruned. \n",
            "[I 2025-04-03 07:20:12,843] Trial 329 pruned. \n",
            "[I 2025-04-03 07:20:14,424] Trial 330 pruned. \n",
            "[I 2025-04-03 07:20:34,209] Trial 331 finished with value: 0.7883817427385892 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 4, 'kernel_size_2': 10, 'kernel_size_3': 4, 'kernel_size_4': 12, 'kernel_size_5': 7, 'kernel_size_6': 13, 'kernel_size_7': 8, 'kernel_size_8': 9, 'kernel_size_9': 3, 'kernel_size_10': 13, 'kernel_size_11': 6, 'output_channels_0': 8, 'output_channels_1': 32, 'output_channels_2': 24, 'output_channels_3': 24, 'output_channels_4': 20, 'output_channels_5': 16, 'output_channels_6': 24, 'output_channels_7': 8, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 4, 'output_channels_11': 16}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:20:34,881] Trial 332 pruned. \n",
            "[I 2025-04-03 07:20:35,541] Trial 333 pruned. \n",
            "[I 2025-04-03 07:20:36,202] Trial 334 pruned. \n",
            "[I 2025-04-03 07:20:36,864] Trial 335 pruned. \n",
            "[I 2025-04-03 07:20:37,552] Trial 336 pruned. \n",
            "[I 2025-04-03 07:20:58,063] Trial 337 finished with value: 0.7759336099585062 and parameters: {'kernel_size_0': 11, 'kernel_size_1': 4, 'kernel_size_2': 11, 'kernel_size_3': 4, 'kernel_size_4': 11, 'kernel_size_5': 7, 'kernel_size_6': 12, 'kernel_size_7': 8, 'kernel_size_8': 9, 'kernel_size_9': 3, 'kernel_size_10': 12, 'kernel_size_11': 6, 'output_channels_0': 8, 'output_channels_1': 32, 'output_channels_2': 20, 'output_channels_3': 24, 'output_channels_4': 20, 'output_channels_5': 16, 'output_channels_6': 24, 'output_channels_7': 8, 'output_channels_8': 20, 'output_channels_9': 24, 'output_channels_10': 4, 'output_channels_11': 16}. Best is trial 34 with value: 0.8257261410788381.\n",
            "[I 2025-04-03 07:20:58,718] Trial 338 pruned. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test new model\n",
        "# @title\n",
        "#Best acc = 83.4% (42 epochs)\n",
        "parallel_channels =  6\n",
        "num_conv_layers =  2\n",
        "kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "drop = 0.2915637951496211\n",
        "linear_layers = 0\n",
        "lr =  0.008900505464977875\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs = 42  # Adjust based on early stopping\n",
        "# kernel_size_0': 4, 'kernel_size_1': 7, 'kernel_size_2': 6,\n",
        "# 'kernel_size_3': 5, 'kernel_size_4': 6, 'kernel_size_5': 4,\n",
        "# 'kernel_size_6': 10, 'kernel_size_7': 6, 'kernel_size_8': 13,\n",
        "# 'kernel_size_9': 4, 'kernel_size_10': 15, 'kernel_size_11': 7,\n",
        "# 'output_channels_0': 8, 'output_channels_1': 16,\n",
        "# 'output_channels_2': 4, 'output_channels_3': 4,\n",
        "# 'output_channels_4': 4, 'output_channels_5': 28,\n",
        "# 'output_channels_6': 28, 'output_channels_7': 4,\n",
        "# 'output_channels_8': 8, 'output_channels_9': 8,\n",
        "# 'output_channels_10': 8, 'output_channels_11': 24\n",
        "\n",
        "class TestNet(nn.Module):\n",
        "  def __init__(self, input_channels, input_length, num_classes, drop):\n",
        "        super(TestNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.input_length = input_length\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 12, kernel_size=4, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(12, 32, kernel_size=7, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 24, kernel_size=6, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(24, 12, kernel_size=5, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 8, kernel_size=6, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(8, 8, kernel_size=4, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch5 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=13, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(16, 8, kernel_size=4, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 20, kernel_size=10, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(20, 4, kernel_size=6, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        self.branch6 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 12, kernel_size=15, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(12, 4, kernel_size=7, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            example_input = torch.randn(1, input_channels, input_length)\n",
        "            out1 = self.branch1(example_input)\n",
        "            out2 = self.branch2(example_input)\n",
        "            out3 = self.branch3(example_input)\n",
        "            out4 = self.branch4(example_input)\n",
        "            out5 = self.branch5(example_input)\n",
        "            out6 = self.branch6(example_input)\n",
        "            self.fc_input_dim = out1.numel() + out2.numel() + out3.numel() + out4.numel() + out5.numel() + out6.numel()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.fc_input_dim, num_classes),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "        out1 = self.branch1(x)\n",
        "        out2 = self.branch2(x)\n",
        "        out3 = self.branch3(x)\n",
        "        out4 = self.branch4(x)\n",
        "        out5 = self.branch5(x)\n",
        "        out6 = self.branch6(x)\n",
        "        combined = torch.cat([out1.flatten(1), out2.flatten(1), out3.flatten(1), out4.flatten(1), out5.flatten(1), out6.flatten(1)], dim=1)\n",
        "        return F.softmax(self.fc(combined), dim = -1)\n",
        "\n",
        "model = TestNet(16, 80, 2, 0.2915637951496211).to(device)\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.008900505464977875)\n",
        "model.eval()\n",
        "\n",
        "parallel_channels =  6\n",
        "num_conv_layers =  2\n",
        "kernel_size = [11, 3, 9, 2, 13, 7, 5, 3, 12, 6, 15, 8]\n",
        "output_channels = [12, 32, 24, 12, 8, 8, 20, 4, 16, 8, 12, 4]\n",
        "drop = 0.2915637951496211\n",
        "linear_layers = 0\n",
        "lr =  0.008900505464977875\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "num_epochs = 42\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "\n",
        "for epoch in range(30):\n",
        "    # Training phase\n",
        "    # model.train()\n",
        "    # epoch_train_loss, correct_train, total_train = 0, 0, 0\n",
        "    # for batch, (data, targets) in enumerate(train_loader):\n",
        "    #     data, targets = data.to(device), targets.to(device)\n",
        "    #     outputs = model(data)\n",
        "\n",
        "    #     # Add these checks\n",
        "    #     if torch.isnan(outputs).any():\n",
        "    #         print(\"NaN in model outputs!\")\n",
        "    #         break\n",
        "    #     loss = criterion(outputs, targets)\n",
        "\n",
        "    #     optimizer.zero_grad()\n",
        "    #     loss.backward()\n",
        "    #     optimizer.step()\n",
        "\n",
        "    #     # Track metrics\n",
        "    #     epoch_train_loss += loss.item()\n",
        "    #     _, predicted = torch.max(outputs.data, 1)\n",
        "    #     total_train += targets.size(0)\n",
        "    #     correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    epoch_val_loss, correct_val, total_val = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (data, targets) in enumerate(val_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            epoch_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += targets.size(0)\n",
        "            correct_val += (predicted == targets).sum().item()\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    train_loss = epoch_train_loss / len(train_loader)\n",
        "    val_loss = epoch_val_loss / len(val_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    val_acc = correct_val / total_val\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "    print(\"----------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "collapsed": true,
        "id": "ipAA3mO_t9x-",
        "outputId": "9cb94bb6-9bf6-4f70-a18f-f46b332bb1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for TestNet:\n\tMissing key(s) in state_dict: \"branch1.0.weight\", \"branch1.0.bias\", \"branch1.4.weight\", \"branch1.4.bias\", \"branch2.0.weight\", \"branch2.0.bias\", \"branch2.4.weight\", \"branch2.4.bias\", \"branch3.0.weight\", \"branch3.0.bias\", \"branch3.4.weight\", \"branch3.4.bias\", \"branch5.0.weight\", \"branch5.0.bias\", \"branch5.4.weight\", \"branch5.4.bias\", \"branch4.0.weight\", \"branch4.0.bias\", \"branch4.4.weight\", \"branch4.4.bias\", \"branch6.0.weight\", \"branch6.0.bias\", \"branch6.4.weight\", \"branch6.4.bias\". \n\tUnexpected key(s) in state_dict: \"parallels.0.0.weight\", \"parallels.0.0.bias\", \"parallels.0.1.weight\", \"parallels.0.1.bias\", \"parallels.1.0.weight\", \"parallels.1.0.bias\", \"parallels.1.1.weight\", \"parallels.1.1.bias\", \"parallels.2.0.weight\", \"parallels.2.0.bias\", \"parallels.2.1.weight\", \"parallels.2.1.bias\", \"parallels.3.0.weight\", \"parallels.3.0.bias\", \"parallels.3.1.weight\", \"parallels.3.1.bias\", \"parallels.4.0.weight\", \"parallels.4.0.bias\", \"parallels.4.1.weight\", \"parallels.4.1.bias\", \"parallels.5.0.weight\", \"parallels.5.0.bias\", \"parallels.5.1.weight\", \"parallels.5.1.bias\", \"convs.0.weight\", \"convs.0.bias\", \"convs.1.weight\", \"convs.1.bias\". \n\tsize mismatch for fc.0.weight: copying a param with shape torch.Size([2, 1744]) from checkpoint, the shape in current model is torch.Size([2, 2720]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5ac4b6a93ba6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2915637951496211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.008900505464977875\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TestNet:\n\tMissing key(s) in state_dict: \"branch1.0.weight\", \"branch1.0.bias\", \"branch1.4.weight\", \"branch1.4.bias\", \"branch2.0.weight\", \"branch2.0.bias\", \"branch2.4.weight\", \"branch2.4.bias\", \"branch3.0.weight\", \"branch3.0.bias\", \"branch3.4.weight\", \"branch3.4.bias\", \"branch5.0.weight\", \"branch5.0.bias\", \"branch5.4.weight\", \"branch5.4.bias\", \"branch4.0.weight\", \"branch4.0.bias\", \"branch4.4.weight\", \"branch4.4.bias\", \"branch6.0.weight\", \"branch6.0.bias\", \"branch6.4.weight\", \"branch6.4.bias\". \n\tUnexpected key(s) in state_dict: \"parallels.0.0.weight\", \"parallels.0.0.bias\", \"parallels.0.1.weight\", \"parallels.0.1.bias\", \"parallels.1.0.weight\", \"parallels.1.0.bias\", \"parallels.1.1.weight\", \"parallels.1.1.bias\", \"parallels.2.0.weight\", \"parallels.2.0.bias\", \"parallels.2.1.weight\", \"parallels.2.1.bias\", \"parallels.3.0.weight\", \"parallels.3.0.bias\", \"parallels.3.1.weight\", \"parallels.3.1.bias\", \"parallels.4.0.weight\", \"parallels.4.0.bias\", \"parallels.4.1.weight\", \"parallels.4.1.bias\", \"parallels.5.0.weight\", \"parallels.5.0.bias\", \"parallels.5.1.weight\", \"parallels.5.1.bias\", \"convs.0.weight\", \"convs.0.bias\", \"convs.1.weight\", \"convs.1.bias\". \n\tsize mismatch for fc.0.weight: copying a param with shape torch.Size([2, 1744]) from checkpoint, the shape in current model is torch.Size([2, 2720])."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}